<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.main API documentation</title>
<meta name="description" content="Main file to train and evaluate models with distributed optimizers." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.main</code></h1>
</header>
<section id="section-intro">
<p>Main file to train and evaluate models with distributed optimizers.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Main file to train and evaluate models with distributed optimizers.&#34;&#34;&#34;


import os

from typing import List

import argparse

import numpy as np

import torch
import torch.distributed as dist
import torch.multiprocessing as mp


import utils as ut
from datasets import PartitionGenerator
from dist_training import train_eval_dist


def run(rank: int, world_size: int, blind_list: List[int], byz_list: List[int], seed: int,
        data_type: str, model_name: str, optim_name: str, lr: float = 1e-3,
        lr_decay_step: int = 30, lr_decay_rate: float = 0.1, momentum: float = 0.0,
        weight_decay: float = 0.0, n_epochs: int = 10, task: str = &#34;classification&#34;,
        save_loss: bool = True, save_acc: bool = True, folder_name: str = &#34;tmp&#34;,
        save_step: int = 10, verbose: int = 1):
    &#34;&#34;&#34;Run experiment.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    blind_list : list of int
        Ranks of blind adversaries inverting their gradients signs.

    byz_list : list of int
        Ranks of Byzantine adversaries.

    seed : int
        Seed to use everywhere for reproducibility.

    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    lr : float, default=0.0001
        Learning rate for the optimizer.

    lr_decay_step : int, default=30
        Number of steps between each modification of the learning rate.

    lr_decay_rate : float, default=0.1
        Value by which the learning rate is multiplied every *lr_decay_step*.

    momentum : float, default=0.0
        Momentum for the optimizer.

    weight_decay : float, default=0.0
        Weight decay for the optimizer.

    n_epochs : int, default=10
        Number of training epochs.

    task : {&#34;classification&#34;, &#34;regression&#34;}, default=&#34;classification&#34;
        If &#34;classification&#34; task, will compute the accuracy, otherwise only the loss.

    save_loss : bool, default=True
        If True, training and test loss for each epoch and rank will be saved in csv files.

    save_acc : bool, default=True
        If True, training and test accuracy for each epoch and rank will be saved in csv files.

    save_step : int, default=10
        Number of steps between each save of the metrics. A smaller step is convenient for longer
        training in order not to lose all the work in case of early stopping.

    folder_name : str, default=&#34;tmp&#34;
        Specific name for the folder to save scores results.

    verbose : {0, 1, 2}, default=1
        If 2, will print all metrics of all processes along epochs. If 1, will only show a
        progress bar for the rank 0 process with server&#39;s scores. If 0, will only show a simple
        progress bar.
    &#34;&#34;&#34;
    # Initialize the process
    os.environ[&#34;MASTER_ADDR&#34;] = &#34;localhost&#34;
    os.environ[&#34;MASTER_PORT&#34;] = &#34;12350&#34;

    dist.init_process_group(&#34;gloo&#34;, rank=rank, world_size=world_size)

    # Fix seed
    ut.set_seed(seed)

    # Get training and test sets
    batch_size = int(128 / float(world_size))  # prevent too big batches

    data = PartitionGenerator(data_type, shuffle=True)
    trainset, testset = data.get_partition(rank, world_size)

    # Model &amp; Optimizer initialization
    model = ut.init_model(model_name)
    loss_fn = ut.init_loss(data_type)
    optimizer = ut.init_optim(optim_name, model, blind_list, byz_list, lr=lr, momentum=momentum,
                              weight_decay=weight_decay)

    # Train the process
    train_eval_dist(rank, world_size, trainset, testset, model, loss_fn, optimizer,
                    lr_decay_step=lr_decay_step, lr_decay_rate=lr_decay_rate,
                    batch_size=batch_size, n_epochs=n_epochs, task=task, save_loss=save_loss,
                    save_acc=save_acc, folder_name=folder_name, save_step=save_step,
                    verbose=verbose)

    # Clean the process
    dist.destroy_process_group()


if __name__ == &#34;__main__&#34;:

    if not torch.distributed.is_available():
        error_msg = &#34;PyTorch support for distributed applications is not enabled on your machine.&#34;
        raise ValueError(error_msg)

    # Command lines
    parser_desc = &#34;&#34;&#34;Main file to train and evaluate models with distributed optimizers.&#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=parser_desc, add_help=False)

    parser.add_argument(&#34;-h&#34;,
                        &#34;--help&#34;,
                        action=&#34;help&#34;,
                        default=argparse.SUPPRESS,
                        help=&#34;If selected, will show the help message and exit.&#34;)

    # Number of processes and adversaries
    parser.add_argument(&#34;nprocs&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Number of processes. This number contains healthy workers as well \
                             as eventual adversaries to be set up with optional arguments.
                             &#34;&#34;&#34;)

    parser.add_argument(&#34;-i&#34;,
                        &#34;--blind-inv&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Number of blind adversaries inverting their gradients signs. \
                             Default: 0.
                             &#34;&#34;&#34;,
                        default=0)

    parser.add_argument(&#34;-b&#34;,
                        &#34;--byzantine&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Number of Byzantine adversaries. This number must not be greater \
                             than the number of processes minus the number of blind adversaries. \
                             Default: 0.
                             &#34;&#34;&#34;,
                        default=0)

    # Seed selection
    parser.add_argument(&#34;-s&#34;,
                        &#34;--seed&#34;,
                        type=int,
                        help=&#34;&#34;&#34;The seed to use everywhere for reproducibility. Default: 42.&#34;&#34;&#34;,
                        default=42)

    # Data type and corresponding neural network &amp; optimizer
    parser.add_argument(&#34;dataset&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             Name of the dataset from &#34;linreg&#34;, &#34;logreg&#34;, &#34;mnist&#34; and &#34;imagenet&#34;. \
                             The &#34;linreg&#34; and &#34;logreg&#34; datasets are generated internally during \
                             the runs and can be used to demonstrate the algorithm. The &#34;mnist&#34; \
                             dataset contains 50,000 training and 10,000 test images of size \
                             28x28x1 representing handwritten digits. The &#34;imagenet&#34; dataset \
                             contains more than 1,000,000 training and 100,000 test images of \
                             1,000 different classes.
                             &#34;&#34;&#34;)

    parser.add_argument(&#34;-n&#34;,
                        &#34;--net&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             Name of the neural network from &#34;linregnet&#34;, &#34;logregnet&#34;, &#34;torchnet&#34;, \
                             &#34;mnistnet&#34;, &#34;resnet18&#34; and &#34;resnet50&#34;. &#34;linregnet&#34; is only \
                             compatible with linreg dataset, as well as &#34;logregnet&#34; with logreg. \
                             &#34;torchnet&#34; and &#34;mnistnet&#34; are compatible with mnist dataset, while \
                             &#34;resnet18&#34; and &#34;resnet50&#34; are compatible with imagenet. Default: \
                             &#34;mnistnet&#34;.
                             &#34;&#34;&#34;,
                        default=&#34;mnistnet&#34;)

    parser.add_argument(&#34;-o&#34;,
                        &#34;--optimizer&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             Name of the optimizer from &#34;distsgd&#34;, &#34;signsgd&#34; and &#34;signum&#34;. \
                             &#34;distsgd&#34; is the default stochastic gradient descent with a \
                             distributed support, &#34;signsgd&#34; is the distributed optimizer detailed \
                             in the paper and &#34;signum&#34; is equivalent to &#34;signsgd&#34; with a momentum \
                             parameter. Default: &#34;signum&#34;.
                             &#34;&#34;&#34;,
                        default=&#34;signum&#34;)

    # Training parameters
    parser.add_argument(&#34;-e&#34;,
                        &#34;--epochs&#34;,
                        type=int,
                        help=&#34;&#34;&#34;Number of training epochs. Default: 10.&#34;&#34;&#34;,
                        default=10)

    parser.add_argument(&#34;--lr&#34;,
                        type=float,
                        help=&#34;&#34;&#34;Learning rate. Default: 0.001.&#34;&#34;&#34;,
                        default=1e-3)

    parser.add_argument(&#34;--lr-decay-step&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Number of steps between each modification of the learning rate. \
                             Default: 30.
                             &#34;&#34;&#34;,
                        default=30)

    parser.add_argument(&#34;--lr-decay-rate&#34;,
                        type=float,
                        help=&#34;&#34;&#34;
                             Value by which the learning rate is multiplied every lr_decay_step. \
                             Default: 0.1.
                             &#34;&#34;&#34;,
                        default=0.1)

    parser.add_argument(&#34;--momentum&#34;,
                        type=float,
                        help=&#34;&#34;&#34;
                             Momentum parameter. Only available for &#34;distsgd&#34; and &#34;signum&#34; \
                             optimizers. Default: 0.
                             &#34;&#34;&#34;,
                        default=0.0)

    parser.add_argument(&#34;--weight-decay&#34;,
                        type=float,
                        help=&#34;&#34;&#34;
                             Weight decay. Only available for &#34;signsgd&#34; and &#34;signum&#34; optimizers. \
                             Default: 0.
                             &#34;&#34;&#34;,
                        default=0.0)

    # Saving results
    parser.add_argument(&#34;--loss&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             If True, training and test loss for each epoch will be saved in csv \
                             files. Verbose argument allows to select if files have to be written \
                             only for the mean scores or for each process. Default: True.
                             &#34;&#34;&#34;,
                        default=&#34;True&#34;)

    parser.add_argument(&#34;--acc&#34;,
                        type=str,
                        help=&#34;&#34;&#34;
                             If True, training and test accuracy for each epoch will be saved in \
                             csv files. Verbose argument allows to select if files have to be \
                             written only for the mean scores or for each process. Default: True.
                             &#34;&#34;&#34;,
                        default=&#34;True&#34;)

    parser.add_argument(&#34;--save-step&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             Number of steps between each save of the metrics. A smaller step is \
                             convenient for longer training in order not to lose all the work in \
                             case of early stopping. Default: 10.
                             &#34;&#34;&#34;,
                        default=10)

    parser.add_argument(&#34;-v&#34;,
                        &#34;--verbose&#34;,
                        type=int,
                        help=&#34;&#34;&#34;
                             If 2, will print all metrics of all processes along epochs. If 1, \
                             will show a progress bar with the mean scores. If 0, will show a raw \
                             progress bar. If loss or acc saving options are True, a verbose of 1 \
                             will allow writing files only for the mean scores while a verbose of \
                             2 will allow writing files for each process. Default: 1.
                             &#34;&#34;&#34;,
                        default=1)

    # Parse parameters
    args = parser.parse_args()

    seed = args.seed

    world_size = args.nprocs
    blind_size = args.blind_inv
    byz_size = args.byzantine

    n_epochs = args.epochs
    lr = args.lr
    lr_decay_step = args.lr_decay_step
    lr_decay_rate = args.lr_decay_rate
    momentum = args.momentum
    weight_decay = args.weight_decay

    data_type = args.dataset.lower()
    model_name = args.net.lower()
    optim_name = args.optimizer.lower()

    save_loss = ut.str2bool(args.loss)
    save_acc = ut.str2bool(args.acc)
    save_step = args.save_step
    verbose = args.verbose

    # Control parameters format and compatibility
    world_size, blind_size, byz_size = ut.check_proc(world_size, blind_size, byz_size)
    data_type, model_name, optim_name, task = ut.check_nn(data_type, model_name, optim_name)

    # Set global seed
    ut.set_seed(seed)

    # Establish which processes are blind adversaries that invert their gradients signs
    blind_list = list(np.random.choice(world_size, size=blind_size, replace=False))

    # Establish which processes are Byzantine adversaries
    byz_list = list(np.random.choice([rank for rank in range(world_size) if rank not in blind_list],
                                     size=byz_size, replace=False))

    # Summary of the experiment
    if verbose in {1, 2}:

        summary = f&#34;In total, there are {world_size} processes,&#34;
        summary += f&#34; of which {blind_size} are blind adversaries&#34;
        summary += f&#34; and {byz_size} are Byzantine adversaries.&#34;
        summary += f&#34; Optimizer is {optim_name}.&#34;
        print(summary)

    # Path parameters
    folder_name = data_type + &#34;_&#34; + model_name + &#34;_&#34; + optim_name

    # Run experiment
    processes = []
    mp.set_start_method(&#34;spawn&#34;)

    for rank in range(world_size):

        run_args = (rank, world_size, blind_list, byz_list, seed, data_type, model_name,
                    optim_name, lr, lr_decay_step, lr_decay_rate, momentum, weight_decay, n_epochs,
                    task, save_loss, save_acc, folder_name, save_step, verbose)
        p = mp.Process(target=run, args=run_args)
        p.start()

        processes.append(p)

    for p in processes:

        p.join()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.main.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>rank: int, world_size: int, blind_list: List[int], byz_list: List[int], seed: int, data_type: str, model_name: str, optim_name: str, lr: float = 0.001, lr_decay_step: int = 30, lr_decay_rate: float = 0.1, momentum: float = 0.0, weight_decay: float = 0.0, n_epochs: int = 10, task: str = 'classification', save_loss: bool = True, save_acc: bool = True, folder_name: str = 'tmp', save_step: int = 10, verbose: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Run experiment.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code></dt>
<dd>Rank of current process.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes.</dd>
<dt><strong><code>blind_list</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>Ranks of blind adversaries inverting their gradients signs.</dd>
<dt><strong><code>byz_list</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>Ranks of Byzantine adversaries.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed to use everywhere for reproducibility.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"logreg", "linreg", "mnist", "imagenet"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"logregnet", "linregnet", "torchnet", "mnistnet", "resnet18", "resnet50"}</code></dt>
<dd>Name of the neural network to use for training.</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"distsgd", "signsgd", "signum"}</code></dt>
<dd>Name of the optimizer.</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code>, default=<code>0.0001</code></dt>
<dd>Learning rate for the optimizer.</dd>
<dt><strong><code>lr_decay_step</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Number of steps between each modification of the learning rate.</dd>
<dt><strong><code>lr_decay_rate</code></strong> :&ensp;<code>float</code>, default=<code>0.1</code></dt>
<dd>Value by which the learning rate is multiplied every <em>lr_decay_step</em>.</dd>
<dt><strong><code>momentum</code></strong> :&ensp;<code>float</code>, default=<code>0.0</code></dt>
<dd>Momentum for the optimizer.</dd>
<dt><strong><code>weight_decay</code></strong> :&ensp;<code>float</code>, default=<code>0.0</code></dt>
<dd>Weight decay for the optimizer.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>Number of training epochs.</dd>
<dt><strong><code>task</code></strong> :&ensp;<code>{"classification", "regression"}</code>, default=<code>"classification"</code></dt>
<dd>If "classification" task, will compute the accuracy, otherwise only the loss.</dd>
<dt><strong><code>save_loss</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True, training and test loss for each epoch and rank will be saved in csv files.</dd>
<dt><strong><code>save_acc</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True, training and test accuracy for each epoch and rank will be saved in csv files.</dd>
<dt><strong><code>save_step</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>Number of steps between each save of the metrics. A smaller step is convenient for longer
training in order not to lose all the work in case of early stopping.</dd>
<dt><strong><code>folder_name</code></strong> :&ensp;<code>str</code>, default=<code>"tmp"</code></dt>
<dd>Specific name for the folder to save scores results.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>{0, 1, 2}</code>, default=<code>1</code></dt>
<dd>If 2, will print all metrics of all processes along epochs. If 1, will only show a
progress bar for the rank 0 process with server's scores. If 0, will only show a simple
progress bar.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(rank: int, world_size: int, blind_list: List[int], byz_list: List[int], seed: int,
        data_type: str, model_name: str, optim_name: str, lr: float = 1e-3,
        lr_decay_step: int = 30, lr_decay_rate: float = 0.1, momentum: float = 0.0,
        weight_decay: float = 0.0, n_epochs: int = 10, task: str = &#34;classification&#34;,
        save_loss: bool = True, save_acc: bool = True, folder_name: str = &#34;tmp&#34;,
        save_step: int = 10, verbose: int = 1):
    &#34;&#34;&#34;Run experiment.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    blind_list : list of int
        Ranks of blind adversaries inverting their gradients signs.

    byz_list : list of int
        Ranks of Byzantine adversaries.

    seed : int
        Seed to use everywhere for reproducibility.

    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    lr : float, default=0.0001
        Learning rate for the optimizer.

    lr_decay_step : int, default=30
        Number of steps between each modification of the learning rate.

    lr_decay_rate : float, default=0.1
        Value by which the learning rate is multiplied every *lr_decay_step*.

    momentum : float, default=0.0
        Momentum for the optimizer.

    weight_decay : float, default=0.0
        Weight decay for the optimizer.

    n_epochs : int, default=10
        Number of training epochs.

    task : {&#34;classification&#34;, &#34;regression&#34;}, default=&#34;classification&#34;
        If &#34;classification&#34; task, will compute the accuracy, otherwise only the loss.

    save_loss : bool, default=True
        If True, training and test loss for each epoch and rank will be saved in csv files.

    save_acc : bool, default=True
        If True, training and test accuracy for each epoch and rank will be saved in csv files.

    save_step : int, default=10
        Number of steps between each save of the metrics. A smaller step is convenient for longer
        training in order not to lose all the work in case of early stopping.

    folder_name : str, default=&#34;tmp&#34;
        Specific name for the folder to save scores results.

    verbose : {0, 1, 2}, default=1
        If 2, will print all metrics of all processes along epochs. If 1, will only show a
        progress bar for the rank 0 process with server&#39;s scores. If 0, will only show a simple
        progress bar.
    &#34;&#34;&#34;
    # Initialize the process
    os.environ[&#34;MASTER_ADDR&#34;] = &#34;localhost&#34;
    os.environ[&#34;MASTER_PORT&#34;] = &#34;12350&#34;

    dist.init_process_group(&#34;gloo&#34;, rank=rank, world_size=world_size)

    # Fix seed
    ut.set_seed(seed)

    # Get training and test sets
    batch_size = int(128 / float(world_size))  # prevent too big batches

    data = PartitionGenerator(data_type, shuffle=True)
    trainset, testset = data.get_partition(rank, world_size)

    # Model &amp; Optimizer initialization
    model = ut.init_model(model_name)
    loss_fn = ut.init_loss(data_type)
    optimizer = ut.init_optim(optim_name, model, blind_list, byz_list, lr=lr, momentum=momentum,
                              weight_decay=weight_decay)

    # Train the process
    train_eval_dist(rank, world_size, trainset, testset, model, loss_fn, optimizer,
                    lr_decay_step=lr_decay_step, lr_decay_rate=lr_decay_rate,
                    batch_size=batch_size, n_epochs=n_epochs, task=task, save_loss=save_loss,
                    save_acc=save_acc, folder_name=folder_name, save_step=save_step,
                    verbose=verbose)

    # Clean the process
    dist.destroy_process_group()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="Directory index" href="index.html">
<img src="logo.png" alt="">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.main.run" href="#src.main.run">run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>