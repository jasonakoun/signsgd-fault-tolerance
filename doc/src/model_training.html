<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.model_training API documentation</title>
<meta name="description" content="Training and evaluation functions are gathered in this file." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.model_training</code></h1>
</header>
<section id="section-intro">
<p>Training and evaluation functions are gathered in this file.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Training and evaluation functions are gathered in this file.&#34;&#34;&#34;


# Import Python packages
import os
import numpy as np
import pandas as pd
import torch
import torch.distributed as dist
from typing import List, Union
from tqdm import tqdm

# Import our own packages
import utils as ut
from nn import TorchNet, MNISTNet, ResNet18, ResNet50, LinRegNet, LogRegNet
from optim import DistSGD, Signum


def train_process(rank: int, world_size: int, blind_inv_adv: np.ndarray, byz_adv: np.ndarray,
                  seed: int, data_type: str, model_name: str, optim_name: str, n_epochs: int = 10,
                  save_score: bool = True, verbose: bool = True):
    &#34;&#34;&#34;
    Setup a process of given **rank** and train the local model.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    blind_inv_adv : numpy.ndarray
        Array containing the ranks of blind adversaries that invert their gradients signs.

    byz_adv : np.ndarray
        Ranks of Byzantine adversaries.

    seed : int
        Global seed.

    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;}
        Name of the optimizer.

    n_epochs : int, default=1
        Number of training epochs.

    save_score : bool, default=True
        If True, scores (loss and accuracy) for each epoch will be saved in csv files.

    verbose : boool, default=True
        If True, will print all metrics of all processes along epochs. If False, will only show a
        progress bar for the rank 0 process.

    Raises
    ------
    ValueError
        If the name of the model is not contained in the specified choices.
    &#34;&#34;&#34;
    # Initialize the process
    setup(rank, world_size)

    # Fix seed
    ut.set_seed(seed)

    # Start training
    bs = int(128 / float(world_size))  # prevent too big batches

    train_set, test_set = ut.build_train_test_set(rank, world_size, data_type, bs, shuffle=True)

    model: Union[TorchNet, MNISTNet, ResNet18, ResNet50, LinRegNet]
    if model_name == &#34;TorchNet&#34;:
        model = TorchNet(10)
    elif model_name == &#34;MNISTNet&#34;:
        model = MNISTNet(10)
    elif model_name == &#34;ResNet18&#34;:
        model = ResNet18(1000)
    elif model_name == &#34;ResNet50&#34;:
        model = ResNet50(1000)
    elif model_name == &#34;LinRegNet&#34;:
        model = LinRegNet()
    elif model_name == &#34;LogRegNet&#34;:
        model = LogRegNet()
    else:
        raise ValueError(&#34;Unsupported nn {} in training function.&#34;.format(model_name))

    optimizer: Union[DistSGD, Signum]
    if optim_name == &#34;DistSGD&#34;:
        optimizer = DistSGD(model.parameters(), blind_inv_adv, byz_adv, lr=1e-4)
    elif optim_name == &#34;Signum&#34;:
        optimizer = Signum(model.parameters(), blind_inv_adv, byz_adv, lr=1e-4, momentum=0.9,
                           weight_decay=0.)
    elif optim_name == &#34;SignSGD&#34;:
        optimizer = Signum(model.parameters(), blind_inv_adv, byz_adv, lr=1e-4, momentum=0.,
                           weight_decay=0.)
    else:
        raise ValueError(&#34;Unsupported optimizer {} in training function.&#34;.format(optim_name))

    loss_fn: Union[torch.nn.MSELoss, torch.nn.NLLLoss]
    if data_type == &#34;LinReg&#34;:
        loss_fn = torch.nn.MSELoss()
    elif data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
        loss_fn = torch.nn.NLLLoss()
    else:
        raise ValueError(&#34;No loss function for dataset {} in training function.&#34;.format(data_type))

    train_loss_value = 0.
    train_loss_list: List[float] = []
    test_loss_value = 0.
    test_loss_list = []

    train_loss_epoch = 0.
    test_loss_epoch = 0.
    loss_list = []

    train_acc_value = 0.
    train_acc_list = []
    test_acc_value = 0.
    test_acc_list = []

    train_acc_epoch = 0.
    test_acc_epoch = 0.
    acc_list = []

    if verbose or rank == 0:
        values = (rank, world_size-1, 0, 0.)
        pbar = tqdm(range(n_epochs), desc=&#34;Rank {}/{}; Epoch {}; Test loss {}&#34;.format(*values))
    else:
        pbar = range(n_epochs)
    for epoch in pbar:

        # Training step
        for x_train, y_train in train_set:

            # Setting grad to zero
            optimizer.zero_grad()

            # Model output &amp; loss
            y_lsm_pred = model(x_train)
            # print(y_lsm_pred, y_train.flatten())
            if data_type == &#34;LinReg&#34;:
                train_loss = loss_fn(y_lsm_pred.flatten(), y_train.flatten())
            else:
                train_loss = loss_fn(y_lsm_pred, y_train.flatten())

            # Compute scores on training
            train_loss_value = train_loss.item()

            train_loss_list.append(train_loss_value)

            if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
                y_pred = torch.argmax(y_lsm_pred, dim=1)
                train_acc_value = (y_pred == y_train).float().mean()
                train_acc_list.append(train_acc_value)

            # Optimization step
            train_loss.backward()

            optimizer.step()

        # After each epoch of training, we evaluate the model, which is common to all processes
        if rank == 0:
            for x_test, y_test in test_set:

                # Model output &amp; loss
                y_lsm_pred = model(x_test)

                if data_type == &#34;LinReg&#34;:
                    test_loss = loss_fn(y_lsm_pred.flatten(), y_test.flatten())
                else:
                    test_loss = loss_fn(y_lsm_pred, y_test.flatten())

                # Compute scores on test
                test_loss_value = test_loss.item()
                test_loss_list.append(test_loss_value)

                if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
                    y_pred = torch.argmax(y_lsm_pred, dim=1)
                    test_acc_value = (y_pred == y_test).float().mean()
                    test_acc_list.append(test_acc_value)

            test_loss_epoch = np.mean(test_loss_list)
            test_loss_list = []

            if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
                test_acc_epoch = np.mean(test_acc_list)
                test_acc_list = []

        dist.barrier()  # ensure every process are here after evaluation

        train_loss_epoch = np.mean(train_loss_list)
        train_loss_list = []
        loss_list.append([epoch+1, train_loss_epoch, test_loss_epoch])

        if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
            train_acc_epoch = np.mean(train_acc_list)
            train_acc_list = []
            acc_list.append([epoch+1, train_acc_epoch, test_acc_epoch])

        if verbose:
            msg_worker = &#34;Rank {}/{}; Epoch {}; Training loss {}; Test loss {}; Test accuracy {}&#34;
            v_worker = (rank, world_size, epoch, train_loss_epoch, test_loss_epoch, test_acc_epoch)
            print(msg_worker.format(*v_worker))

        if verbose or rank == 0:
            msg_server = &#34;Rank {}/{}; Epoch {}; Test loss {}; Test accuracy {}&#34;
            v_server = (rank, world_size-1, epoch, test_loss_epoch, test_acc_epoch)
            pbar.set_description(msg_server.format(*v_server))

    # Saving scores
    if save_score:

        loss_df = pd.DataFrame(loss_list, columns=[&#34;Epoch&#34;, &#34;Training loss&#34;, &#34;Test loss&#34;])
        acc_df = pd.DataFrame(acc_list, columns=[&#34;Epoch&#34;, &#34;Training accuracy&#34;, &#34;Test accuracy&#34;])

        # Create necessary folders
        byz_adv_size = len(byz_adv)
        blind_inv_size = len(blind_inv_adv)

        scores_folder = &#34;results/&#34;

        if rank == 0:
            if not os.path.exists(scores_folder):
                os.makedirs(scores_folder)
        dist.barrier()

        scores_folder += data_type + &#34;_&#34; + model_name + &#34;_&#34; + optim_name + &#34;/&#34;

        if rank == 0:
            if not os.path.exists(scores_folder):
                os.makedirs(scores_folder)
        dist.barrier()

        scores_folder += &#34;n&#34; + str(world_size) + &#34;_byz&#34; + str(byz_adv_size) + &#34;_inv&#34; + \
            str(blind_inv_size) + &#34;/&#34;

        if rank == 0:
            if not os.path.exists(scores_folder):
                os.makedirs(scores_folder)
        dist.barrier()

        path_to_loss = scores_folder + &#34;loss_p&#34; + str(rank) + &#34;.csv&#34;
        loss_df.to_csv(path_to_loss, columns=loss_df.columns, header=True, index=False)

        # Save data
        if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
            path_to_acc = scores_folder + &#34;accuracy_p&#34; + str(rank) + &#34;.csv&#34;
            acc_df.to_csv(path_to_acc, columns=acc_df.columns, header=True, index=False)

    # Clean the process
    cleanup()


def setup(rank: int, world_size: int, backend: str = &#34;gloo&#34;):
    &#34;&#34;&#34;
    Initialize the distributed environment.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    backend : str, default=&#34;gloo&#34;
        Backend to use in the distributed environment.
    &#34;&#34;&#34;
    os.environ[&#34;MASTER_ADDR&#34;] = &#34;localhost&#34;
    os.environ[&#34;MASTER_PORT&#34;] = &#34;12355&#34;

    dist.init_process_group(backend, rank=rank, world_size=world_size)


def cleanup():
    &#34;&#34;&#34;Clean the current process group.&#34;&#34;&#34;
    dist.destroy_process_group()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.model_training.cleanup"><code class="name flex">
<span>def <span class="ident">cleanup</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Clean the current process group.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup():
    &#34;&#34;&#34;Clean the current process group.&#34;&#34;&#34;
    dist.destroy_process_group()</code></pre>
</details>
</dd>
<dt id="src.model_training.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>rank: int, world_size: int, backend: str = 'gloo')</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the distributed environment.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code></dt>
<dd>Rank of current process.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes.</dd>
<dt><strong><code>backend</code></strong> :&ensp;<code>str</code>, default=<code>"gloo"</code></dt>
<dd>Backend to use in the distributed environment.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(rank: int, world_size: int, backend: str = &#34;gloo&#34;):
    &#34;&#34;&#34;
    Initialize the distributed environment.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    backend : str, default=&#34;gloo&#34;
        Backend to use in the distributed environment.
    &#34;&#34;&#34;
    os.environ[&#34;MASTER_ADDR&#34;] = &#34;localhost&#34;
    os.environ[&#34;MASTER_PORT&#34;] = &#34;12355&#34;

    dist.init_process_group(backend, rank=rank, world_size=world_size)</code></pre>
</details>
</dd>
<dt id="src.model_training.train_process"><code class="name flex">
<span>def <span class="ident">train_process</span></span>(<span>rank: int, world_size: int, blind_inv_adv: numpy.ndarray, byz_adv: numpy.ndarray, seed: int, data_type: str, model_name: str, optim_name: str, n_epochs: int = 10, save_score: bool = True, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup a process of given <strong>rank</strong> and train the local model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code></dt>
<dd>Rank of current process.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes.</dd>
<dt><strong><code>blind_inv_adv</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Array containing the ranks of blind adversaries that invert their gradients signs.</dd>
<dt><strong><code>byz_adv</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Ranks of Byzantine adversaries.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Global seed.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"MNIST", "ImageNet", "LinReg", "LogReg"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"TorchNet", "MNISTNet", "ResNet18", "ResNet50", "LinRegNet", "LogRegNet"}</code></dt>
<dd>Name of the neural network to use for training. "TorchNet" and "MNISTNet" are compatible
with "MNIST" dataset, while "ResNetX" are compatible with "ImageNet". "LinRegNet" is only
compatible with "LinReg" dataset, as well as "LogRegNet" and "LogReg".</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"DistSGD", "Signum"}</code></dt>
<dd>Name of the optimizer.</dd>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>Number of training epochs.</dd>
<dt><strong><code>save_score</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True, scores (loss and accuracy) for each epoch will be saved in csv files.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boool</code>, default=<code>True</code></dt>
<dd>If True, will print all metrics of all processes along epochs. If False, will only show a
progress bar for the rank 0 process.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the name of the model is not contained in the specified choices.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_process(rank: int, world_size: int, blind_inv_adv: np.ndarray, byz_adv: np.ndarray,
                  seed: int, data_type: str, model_name: str, optim_name: str, n_epochs: int = 10,
                  save_score: bool = True, verbose: bool = True):
    &#34;&#34;&#34;
    Setup a process of given **rank** and train the local model.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    blind_inv_adv : numpy.ndarray
        Array containing the ranks of blind adversaries that invert their gradients signs.

    byz_adv : np.ndarray
        Ranks of Byzantine adversaries.

    seed : int
        Global seed.

    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;}
        Name of the optimizer.

    n_epochs : int, default=1
        Number of training epochs.

    save_score : bool, default=True
        If True, scores (loss and accuracy) for each epoch will be saved in csv files.

    verbose : boool, default=True
        If True, will print all metrics of all processes along epochs. If False, will only show a
        progress bar for the rank 0 process.

    Raises
    ------
    ValueError
        If the name of the model is not contained in the specified choices.
    &#34;&#34;&#34;
    # Initialize the process
    setup(rank, world_size)

    # Fix seed
    ut.set_seed(seed)

    # Start training
    bs = int(128 / float(world_size))  # prevent too big batches

    train_set, test_set = ut.build_train_test_set(rank, world_size, data_type, bs, shuffle=True)

    model: Union[TorchNet, MNISTNet, ResNet18, ResNet50, LinRegNet]
    if model_name == &#34;TorchNet&#34;:
        model = TorchNet(10)
    elif model_name == &#34;MNISTNet&#34;:
        model = MNISTNet(10)
    elif model_name == &#34;ResNet18&#34;:
        model = ResNet18(1000)
    elif model_name == &#34;ResNet50&#34;:
        model = ResNet50(1000)
    elif model_name == &#34;LinRegNet&#34;:
        model = LinRegNet()
    elif model_name == &#34;LogRegNet&#34;:
        model = LogRegNet()
    else:
        raise ValueError(&#34;Unsupported nn {} in training function.&#34;.format(model_name))

    optimizer: Union[DistSGD, Signum]
    if optim_name == &#34;DistSGD&#34;:
        optimizer = DistSGD(model.parameters(), blind_inv_adv, byz_adv, lr=1e-4)
    elif optim_name == &#34;Signum&#34;:
        optimizer = Signum(model.parameters(), blind_inv_adv, byz_adv, lr=1e-4, momentum=0.9,
                           weight_decay=0.)
    elif optim_name == &#34;SignSGD&#34;:
        optimizer = Signum(model.parameters(), blind_inv_adv, byz_adv, lr=1e-4, momentum=0.,
                           weight_decay=0.)
    else:
        raise ValueError(&#34;Unsupported optimizer {} in training function.&#34;.format(optim_name))

    loss_fn: Union[torch.nn.MSELoss, torch.nn.NLLLoss]
    if data_type == &#34;LinReg&#34;:
        loss_fn = torch.nn.MSELoss()
    elif data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
        loss_fn = torch.nn.NLLLoss()
    else:
        raise ValueError(&#34;No loss function for dataset {} in training function.&#34;.format(data_type))

    train_loss_value = 0.
    train_loss_list: List[float] = []
    test_loss_value = 0.
    test_loss_list = []

    train_loss_epoch = 0.
    test_loss_epoch = 0.
    loss_list = []

    train_acc_value = 0.
    train_acc_list = []
    test_acc_value = 0.
    test_acc_list = []

    train_acc_epoch = 0.
    test_acc_epoch = 0.
    acc_list = []

    if verbose or rank == 0:
        values = (rank, world_size-1, 0, 0.)
        pbar = tqdm(range(n_epochs), desc=&#34;Rank {}/{}; Epoch {}; Test loss {}&#34;.format(*values))
    else:
        pbar = range(n_epochs)
    for epoch in pbar:

        # Training step
        for x_train, y_train in train_set:

            # Setting grad to zero
            optimizer.zero_grad()

            # Model output &amp; loss
            y_lsm_pred = model(x_train)
            # print(y_lsm_pred, y_train.flatten())
            if data_type == &#34;LinReg&#34;:
                train_loss = loss_fn(y_lsm_pred.flatten(), y_train.flatten())
            else:
                train_loss = loss_fn(y_lsm_pred, y_train.flatten())

            # Compute scores on training
            train_loss_value = train_loss.item()

            train_loss_list.append(train_loss_value)

            if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
                y_pred = torch.argmax(y_lsm_pred, dim=1)
                train_acc_value = (y_pred == y_train).float().mean()
                train_acc_list.append(train_acc_value)

            # Optimization step
            train_loss.backward()

            optimizer.step()

        # After each epoch of training, we evaluate the model, which is common to all processes
        if rank == 0:
            for x_test, y_test in test_set:

                # Model output &amp; loss
                y_lsm_pred = model(x_test)

                if data_type == &#34;LinReg&#34;:
                    test_loss = loss_fn(y_lsm_pred.flatten(), y_test.flatten())
                else:
                    test_loss = loss_fn(y_lsm_pred, y_test.flatten())

                # Compute scores on test
                test_loss_value = test_loss.item()
                test_loss_list.append(test_loss_value)

                if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
                    y_pred = torch.argmax(y_lsm_pred, dim=1)
                    test_acc_value = (y_pred == y_test).float().mean()
                    test_acc_list.append(test_acc_value)

            test_loss_epoch = np.mean(test_loss_list)
            test_loss_list = []

            if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
                test_acc_epoch = np.mean(test_acc_list)
                test_acc_list = []

        dist.barrier()  # ensure every process are here after evaluation

        train_loss_epoch = np.mean(train_loss_list)
        train_loss_list = []
        loss_list.append([epoch+1, train_loss_epoch, test_loss_epoch])

        if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
            train_acc_epoch = np.mean(train_acc_list)
            train_acc_list = []
            acc_list.append([epoch+1, train_acc_epoch, test_acc_epoch])

        if verbose:
            msg_worker = &#34;Rank {}/{}; Epoch {}; Training loss {}; Test loss {}; Test accuracy {}&#34;
            v_worker = (rank, world_size, epoch, train_loss_epoch, test_loss_epoch, test_acc_epoch)
            print(msg_worker.format(*v_worker))

        if verbose or rank == 0:
            msg_server = &#34;Rank {}/{}; Epoch {}; Test loss {}; Test accuracy {}&#34;
            v_server = (rank, world_size-1, epoch, test_loss_epoch, test_acc_epoch)
            pbar.set_description(msg_server.format(*v_server))

    # Saving scores
    if save_score:

        loss_df = pd.DataFrame(loss_list, columns=[&#34;Epoch&#34;, &#34;Training loss&#34;, &#34;Test loss&#34;])
        acc_df = pd.DataFrame(acc_list, columns=[&#34;Epoch&#34;, &#34;Training accuracy&#34;, &#34;Test accuracy&#34;])

        # Create necessary folders
        byz_adv_size = len(byz_adv)
        blind_inv_size = len(blind_inv_adv)

        scores_folder = &#34;results/&#34;

        if rank == 0:
            if not os.path.exists(scores_folder):
                os.makedirs(scores_folder)
        dist.barrier()

        scores_folder += data_type + &#34;_&#34; + model_name + &#34;_&#34; + optim_name + &#34;/&#34;

        if rank == 0:
            if not os.path.exists(scores_folder):
                os.makedirs(scores_folder)
        dist.barrier()

        scores_folder += &#34;n&#34; + str(world_size) + &#34;_byz&#34; + str(byz_adv_size) + &#34;_inv&#34; + \
            str(blind_inv_size) + &#34;/&#34;

        if rank == 0:
            if not os.path.exists(scores_folder):
                os.makedirs(scores_folder)
        dist.barrier()

        path_to_loss = scores_folder + &#34;loss_p&#34; + str(rank) + &#34;.csv&#34;
        loss_df.to_csv(path_to_loss, columns=loss_df.columns, header=True, index=False)

        # Save data
        if data_type in {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LogReg&#34;}:
            path_to_acc = scores_folder + &#34;accuracy_p&#34; + str(rank) + &#34;.csv&#34;
            acc_df.to_csv(path_to_acc, columns=acc_df.columns, header=True, index=False)

    # Clean the process
    cleanup()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.model_training.cleanup" href="#src.model_training.cleanup">cleanup</a></code></li>
<li><code><a title="src.model_training.setup" href="#src.model_training.setup">setup</a></code></li>
<li><code><a title="src.model_training.train_process" href="#src.model_training.train_process">train_process</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>