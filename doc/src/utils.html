<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.utils API documentation</title>
<meta name="description" content="Gather utilitary functions for randomness control and data handling." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.utils</code></h1>
</header>
<section id="section-intro">
<p>Gather utilitary functions for randomness control and data handling.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Gather utilitary functions for randomness control and data handling.&#34;&#34;&#34;


# Import Python packages
from typing import List, Tuple
import argparse
import numpy as np
import torch
from numpy.random import multivariate_normal, normal, randn
from scipy.linalg.special_matrices import toeplitz
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms


class LinRegDataset(Dataset):
    &#34;&#34;&#34;Dataset of generated linear regression data.&#34;&#34;&#34;

    def __init__(self, data: Tuple[np.ndarray, ...]):
        &#34;&#34;&#34;
        Initialize a dataset of linear regression data.

        Parameters
        ----------
        data : Tuple[np.ndarray, ...]
            Tuple containing the samples and corresponding labels.
        &#34;&#34;&#34;
        X, y = data
        self.X = torch.from_numpy(X.astype(np.float32))
        self.y = torch.from_numpy(y.astype(np.float32))

        self.transform = None

    def __len__(self) -&gt; int:
        &#34;&#34;&#34;
        Return the length of the dataset.

        Returns
        -------
        length : int
            Length of the dataset.
        &#34;&#34;&#34;
        length = len(self.y)

        return length

    def __getitem__(self, index) -&gt; Tuple[torch.Tensor, ...]:
        &#34;&#34;&#34;
        Return data point(s) at position **idx**.

        Parameters
        ----------
        index : Union[int, List[int]]
            Index or list of indices of selected data points.
        &#34;&#34;&#34;
        samples = self.X[index]
        labels = self.y[index].flatten()

        if self.transform:
            samples = self.transform(samples)
            labels = self.transform(labels)

        return samples, labels


class LogRegDataset(Dataset):
    &#34;&#34;&#34;Dataset of generated logistic regression data.&#34;&#34;&#34;

    def __init__(self, data: Tuple[np.ndarray, ...]):
        &#34;&#34;&#34;
        Initialize a dataset of logistic regression data.

        Parameters
        ----------
        data : Tuple[np.ndarray, ...]
            Tuple containing the samples and corresponding labels.
        &#34;&#34;&#34;
        X, y = data
        self.X = torch.from_numpy(X.astype(np.float32))
        self.y = torch.from_numpy(y)

        self.transform = None

    def __len__(self) -&gt; int:
        &#34;&#34;&#34;
        Return the length of the dataset.

        Returns
        -------
        length : int
            Length of the dataset.
        &#34;&#34;&#34;
        length = len(self.y)

        return length

    def __getitem__(self, index) -&gt; Tuple[torch.Tensor, ...]:
        &#34;&#34;&#34;
        Return data point(s) at position **idx**.

        Parameters
        ----------
        index : Union[int, List[int]]
            Index or list of indices of selected data points.
        &#34;&#34;&#34;
        samples = self.X[index]
        labels = self.y[index].flatten()

        if self.transform:
            samples = self.transform(samples)
            labels = self.transform(labels)

        return samples, labels


class Partition(Dataset):
    &#34;&#34;&#34;Wrapper around a dataset and indices for a partition.&#34;&#34;&#34;

    def __init__(self, data: Dataset, indices: List[int]):
        &#34;&#34;&#34;
        Initialize a partition of a dataset.

        Parameters
        ----------
        data : `torch.utils.data.Dataset`
            Complete dataset.

        indices : List[int]
            List of indices of the dataset that can be used in this partition.
        &#34;&#34;&#34;
        self.data = data
        self.indices = indices

    def __len__(self) -&gt; int:
        &#34;&#34;&#34;
        Return the length of the partition.

        Returns
        -------
        length : int
            Length of the partition.
        &#34;&#34;&#34;
        length = len(self.indices)
        return length

    def __getitem__(self, index) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Return data point(s) at position **index**.

        Parameters
        ----------
        index : Union[int, List[int]]
            Index or list of indices of selected data points.
        &#34;&#34;&#34;
        data_idx = self.indices[index]
        return self.data[data_idx]


def set_seed(seed):
    &#34;&#34;&#34;
    Fix seed for current run.

    Parameters
    ----------
    seed : int
        Global seeed.
    &#34;&#34;&#34;
    np.random.seed(seed)
    torch.manual_seed(seed)


def build_train_test_set(rank: int, world_size: int, data_type: str, batch_size: int,
                         shuffle: bool = True) -&gt; Tuple[DataLoader, ...]:
    &#34;&#34;&#34;
    Create the training and test sets for specified **data_type** name.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    batch_size : int
        Batch size.

    shuffle : bool, default=True
        If True, data will be shuffled in both training and test set.

    Returns
    -------
    train_set : `torch.utils.data.DataLoader`
        Training set built from a partition of the complete dataset.

    test_set : `torch.utils.data.DataLoader`
        Test set.
    &#34;&#34;&#34;
    # Retrieve training and test sets
    if data_type == &#34;MNIST&#34;:

        train = datasets.MNIST(&#34;data/&#34;, train=True, download=True, transform=transforms.ToTensor())
        test = datasets.MNIST(&#34;data/&#34;, train=False, download=True, transform=transforms.ToTensor())

    elif data_type == &#34;ImageNet&#34;:

        train = datasets.ImageNet(&#34;data/&#34;, split=&#34;train&#34;, download=True,
                                  transform=transforms.ToTensor())
        test = datasets.ImageNet(&#34;data/&#34;, split=&#34;test&#34;, download=True,
                                 transform=transforms.ToTensor())

    elif data_type == &#34;LinReg&#34;:

        X_y_train, X_y_test = simu_linreg()
        train = LinRegDataset(X_y_train)
        test = LinRegDataset(X_y_test)

    elif data_type == &#34;LogReg&#34;:

        X_y_train, X_y_test = simu_logreg()
        train = LogRegDataset(X_y_train)
        test = LogRegDataset(X_y_test)

    else:
        raise ValueError(&#34;Dataset {} not supported in dataset creation.&#34;.format(data_type))

    # Shuffle all the indices, here it is vital that the seed is shared among all processes
    n = len(train)
    all_indices = [i for i in range(n)]
    np.random.shuffle(all_indices)

    # Select the data
    partition_size = n // world_size
    first_idx = (rank*n) // world_size
    last_idx = first_idx + partition_size
    partition_indices = all_indices[first_idx:last_idx]

    partition = Partition(train, partition_indices)

    # DataLoader wrapper
    train_set = DataLoader(partition, batch_size=batch_size, shuffle=True)
    test_set = DataLoader(test, batch_size=batch_size, shuffle=True)

    return (train_set, test_set)


def simu_linreg(n_features: int = 20, n_samples: int = 10000, corr: float = 0.5,
                std: float = 0.5) -&gt; Tuple[Tuple[np.ndarray, ...], ...]:
    &#34;&#34;&#34;
    Simulation of a linear regression model with Gaussian features and a Toeplitz covariance, with
    Gaussian noise.

    Parameters
    ----------
    n_features : int
        Number of features to simulate.

    n_samples : int, default=1000
        Number of samples to simulate.

    corr : float, default=0.5
        Correlation of the features.

    std : float, default=0.5
        Standard deviation of the noise.

    Returns
    -------
    train_set : Tuple[np.ndarray]
        Training data.

    test_set : Tuple[np.ndarray]
        Test data.
    &#34;&#34;&#34;
    # Weight creation
    w0 = normal(loc=2, scale=1, size=n_features)

    # Construction of a covariance matrix
    cov = toeplitz(corr ** np.arange(0, n_features))

    # Simulation of features
    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)

    # Simulation of the labels
    y = X.dot(w0) + std * randn(n_samples)

    # Create training and test sets
    train_size = int(0.8*n_samples)

    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    train_set = (X_train, y_train)
    test_set = (X_test, y_test)

    return train_set, test_set


def sigmoid(t: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Sigmoid function.

    Parameters
    ----------
    t : np.ndarray
        Inputs.

    Returns
    -------
    sig_t : np.ndarray
        Sigmoid of inputs.
    &#34;&#34;&#34;
    sig_t = np.zeros(t.shape)

    # Separate where t is nonnegative
    pos_indices = t &gt; 0

    sig_t[pos_indices] = 1/(1 + np.exp(-t[pos_indices]))
    exp_t = np.exp(t[~pos_indices])
    sig_t[~pos_indices] = exp_t/(1 + exp_t)

    return sig_t


def simu_logreg(n_features: int = 20, n_samples: int = 10000,
                corr: float = 0.5) -&gt; Tuple[Tuple[np.ndarray, ...], ...]:
    &#34;&#34;&#34;
    Simulation of a logistic regression model with Gaussian features and a Toeplitz covariance.

    Parameters
    ----------
    n_features : int
        Number of features to simulate.

    n_samples : int, default=1000
        Number of samples to simulate.

    corr : float, default=0.5
        Correlation of the features.

    Returns
    -------
    train_set : Tuple[np.ndarray]
        Training data.

    test_set : Tuple[np.ndarray]
        Test data.
    &#34;&#34;&#34;
    # Weight creation
    w0 = normal(loc=2, scale=1, size=n_features)

    # Construction of a covariance matrix
    cov = toeplitz(corr ** np.arange(0, n_features))

    # Simulation of features
    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)

    # Simulation of the labels
    p = sigmoid(X.dot(w0))
    y = np.random.binomial(1, p, size=n_samples)

    # Create training and test sets
    train_size = int(0.8*n_samples)

    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    train_set = (X_train, y_train)
    test_set = (X_test, y_test)

    return train_set, test_set


def str2bool(v: str) -&gt; bool:
    &#34;&#34;&#34;
    An easy way to handle boolean options.

    Parameters
    ----------
    v : str
        Argument value.

    Returns
    -------
    str2bool(v) : bool
        Corresponding boolean value, if it exists.

    Raises
    ------
    `argparse.ArgumentTypeError`
        If the entry cannot be converted to a boolean.
    &#34;&#34;&#34;
    if isinstance(v, bool):
        return v
    if v.lower() in (&#34;yes&#34;, &#34;true&#34;, &#34;t&#34;, &#34;y&#34;, &#34;1&#34;):
        return True
    if v.lower() in (&#34;no&#34;, &#34;false&#34;, &#34;f&#34;, &#34;n&#34;, &#34;0&#34;):
        return False
    raise argparse.ArgumentTypeError(&#34;Boolean value expected.&#34;)


def restricted_float(x: str) -&gt; float:
    &#34;&#34;&#34;
    An easy war to handle range for floats.

    Parameters
    ----------
    x : float
        Argument value.

    Returns
    -------
    y : float
        Terminal value if **x** is between zero and one.

    Raises
    ------
    `argparse.ArgumentTypeError`
        If **x** cannot be converted to a float or is not in specified range.
    &#34;&#34;&#34;
    try:
        y = float(x)
    except ValueError:
        raise argparse.ArgumentTypeError(&#34;{} not a floating-point literal.&#34;.format(x))

    if y &lt; 0. or y &gt; 1.:
        raise argparse.ArgumentTypeError(&#34;{} not in range [0.0, 1.0].&#34;.format(y))

    return y


def check_params(data_type: str, model_name: str, optim_name: str) -&gt; Tuple[str, ...]:
    &#34;&#34;&#34;
    Check whether chosen parameters are compatible.

    Parameters
    ----------
    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;Signum&#34;}
        Name of the optimizer.

    Returns
    -------
    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;}
        Name of the optimizer.

    Raises
    ------
    ValueError
        If the dataset or model are not supported or compatible.
    &#34;&#34;&#34;
    # Unknown parameters
    available_nn = {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
    if model_name not in available_nn:
        error_msg = &#34;Neural network {} checking error. Please choose one from {}.&#34;
        raise ValueError(error_msg.format(model_name, available_nn))

    available_data = {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
    if data_type not in available_data:
        error_msg = &#34;Dataset {} checking error. Please choose one from {}.&#34;
        raise ValueError(error_msg.format(data_type, available_data))

    availabe_optim = {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;}
    if optim_name not in availabe_optim:
        error_msg = &#34;Optimizer {} is not supported. Please shoose one from {}.&#34;
        raise ValueError(error_msg.format(optim_name, availabe_optim))

    # Incompatible parameters
    if data_type == &#34;MNIST&#34; and model_name not in {&#34;TorchNet&#34;, &#34;MNISTNet&#34;}:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;TorchNet&#34;))
        model_name = &#34;TorchNet&#34;

    if data_type == &#34;ImageNet&#34; and model_name not in {&#34;ResNet18&#34;, &#34;ResNet50&#34;}:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;ResNet18&#34;))
        model_name = &#34;ResNet18&#34;

    if data_type == &#34;LinReg&#34; and model_name != &#34;LinRegNet&#34;:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;LinRegNet&#34;))
        model_name = &#34;LinRegNet&#34;

    if data_type == &#34;LogReg&#34; and model_name != &#34;LogRegNet&#34;:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;LogRegNet&#34;))
        model_name = &#34;LogRegNet&#34;

    return data_type, model_name, optim_name</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.utils.build_train_test_set"><code class="name flex">
<span>def <span class="ident">build_train_test_set</span></span>(<span>rank: int, world_size: int, data_type: str, batch_size: int, shuffle: bool = True) ‑> Tuple[torch.utils.data.dataloader.DataLoader, ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Create the training and test sets for specified <strong>data_type</strong> name.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code></dt>
<dd>Rank of current process.</dd>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"MNIST", "ImageNet", "LinReg", "LogReg"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True, data will be shuffled in both training and test set.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>train_set</code></strong> :&ensp;<code>torch.utils.data.DataLoader</code></dt>
<dd>Training set built from a partition of the complete dataset.</dd>
<dt><strong><code>test_set</code></strong> :&ensp;<code>torch.utils.data.DataLoader</code></dt>
<dd>Test set.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_train_test_set(rank: int, world_size: int, data_type: str, batch_size: int,
                         shuffle: bool = True) -&gt; Tuple[DataLoader, ...]:
    &#34;&#34;&#34;
    Create the training and test sets for specified **data_type** name.

    Parameters
    ----------
    rank : int
        Rank of current process.

    world_size : int
        Total amount of processes.

    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    batch_size : int
        Batch size.

    shuffle : bool, default=True
        If True, data will be shuffled in both training and test set.

    Returns
    -------
    train_set : `torch.utils.data.DataLoader`
        Training set built from a partition of the complete dataset.

    test_set : `torch.utils.data.DataLoader`
        Test set.
    &#34;&#34;&#34;
    # Retrieve training and test sets
    if data_type == &#34;MNIST&#34;:

        train = datasets.MNIST(&#34;data/&#34;, train=True, download=True, transform=transforms.ToTensor())
        test = datasets.MNIST(&#34;data/&#34;, train=False, download=True, transform=transforms.ToTensor())

    elif data_type == &#34;ImageNet&#34;:

        train = datasets.ImageNet(&#34;data/&#34;, split=&#34;train&#34;, download=True,
                                  transform=transforms.ToTensor())
        test = datasets.ImageNet(&#34;data/&#34;, split=&#34;test&#34;, download=True,
                                 transform=transforms.ToTensor())

    elif data_type == &#34;LinReg&#34;:

        X_y_train, X_y_test = simu_linreg()
        train = LinRegDataset(X_y_train)
        test = LinRegDataset(X_y_test)

    elif data_type == &#34;LogReg&#34;:

        X_y_train, X_y_test = simu_logreg()
        train = LogRegDataset(X_y_train)
        test = LogRegDataset(X_y_test)

    else:
        raise ValueError(&#34;Dataset {} not supported in dataset creation.&#34;.format(data_type))

    # Shuffle all the indices, here it is vital that the seed is shared among all processes
    n = len(train)
    all_indices = [i for i in range(n)]
    np.random.shuffle(all_indices)

    # Select the data
    partition_size = n // world_size
    first_idx = (rank*n) // world_size
    last_idx = first_idx + partition_size
    partition_indices = all_indices[first_idx:last_idx]

    partition = Partition(train, partition_indices)

    # DataLoader wrapper
    train_set = DataLoader(partition, batch_size=batch_size, shuffle=True)
    test_set = DataLoader(test, batch_size=batch_size, shuffle=True)

    return (train_set, test_set)</code></pre>
</details>
</dd>
<dt id="src.utils.check_params"><code class="name flex">
<span>def <span class="ident">check_params</span></span>(<span>data_type: str, model_name: str, optim_name: str) ‑> Tuple[str, ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether chosen parameters are compatible.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"MNIST", "ImageNet", "LinReg", "LogReg"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"TorchNet", "MNISTNet", "ResNet18", "ResNet50", "LinRegNet", "LogRegNet"}</code></dt>
<dd>Name of the neural network to use for training. "TorchNet" and "MNISTNet" are compatible
with "MNIST" dataset, while "ResNetX" are compatible with "ImageNet". "LinRegNet" is only
compatible with "LinReg" dataset, as well as "LogRegNet" and "LogReg".</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"DistSGD", "Signum", "Signum"}</code></dt>
<dd>Name of the optimizer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"MNIST", "ImageNet", "LinReg", "LogReg"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"TorchNet", "MNISTNet", "ResNet18", "ResNet50", "LinRegNet", "LogRegNet"}</code></dt>
<dd>Name of the neural network to use for training. "TorchNet" and "MNISTNet" are compatible
with "MNIST" dataset, while "ResNetX" are compatible with "ImageNet". "LinRegNet" is only
compatible with "LinReg" dataset, as well as "LogRegNet" and "LogReg".</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"DistSGD", "Signum", "SignSGD"}</code></dt>
<dd>Name of the optimizer.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the dataset or model are not supported or compatible.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_params(data_type: str, model_name: str, optim_name: str) -&gt; Tuple[str, ...]:
    &#34;&#34;&#34;
    Check whether chosen parameters are compatible.

    Parameters
    ----------
    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;Signum&#34;}
        Name of the optimizer.

    Returns
    -------
    data_type : {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
        Name of the dataset.

    model_name : {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
        Name of the neural network to use for training. &#34;TorchNet&#34; and &#34;MNISTNet&#34; are compatible
        with &#34;MNIST&#34; dataset, while &#34;ResNetX&#34; are compatible with &#34;ImageNet&#34;. &#34;LinRegNet&#34; is only
        compatible with &#34;LinReg&#34; dataset, as well as &#34;LogRegNet&#34; and &#34;LogReg&#34;.

    optim_name : {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;}
        Name of the optimizer.

    Raises
    ------
    ValueError
        If the dataset or model are not supported or compatible.
    &#34;&#34;&#34;
    # Unknown parameters
    available_nn = {&#34;TorchNet&#34;, &#34;MNISTNet&#34;, &#34;ResNet18&#34;, &#34;ResNet50&#34;, &#34;LinRegNet&#34;, &#34;LogRegNet&#34;}
    if model_name not in available_nn:
        error_msg = &#34;Neural network {} checking error. Please choose one from {}.&#34;
        raise ValueError(error_msg.format(model_name, available_nn))

    available_data = {&#34;MNIST&#34;, &#34;ImageNet&#34;, &#34;LinReg&#34;, &#34;LogReg&#34;}
    if data_type not in available_data:
        error_msg = &#34;Dataset {} checking error. Please choose one from {}.&#34;
        raise ValueError(error_msg.format(data_type, available_data))

    availabe_optim = {&#34;DistSGD&#34;, &#34;Signum&#34;, &#34;SignSGD&#34;}
    if optim_name not in availabe_optim:
        error_msg = &#34;Optimizer {} is not supported. Please shoose one from {}.&#34;
        raise ValueError(error_msg.format(optim_name, availabe_optim))

    # Incompatible parameters
    if data_type == &#34;MNIST&#34; and model_name not in {&#34;TorchNet&#34;, &#34;MNISTNet&#34;}:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;TorchNet&#34;))
        model_name = &#34;TorchNet&#34;

    if data_type == &#34;ImageNet&#34; and model_name not in {&#34;ResNet18&#34;, &#34;ResNet50&#34;}:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;ResNet18&#34;))
        model_name = &#34;ResNet18&#34;

    if data_type == &#34;LinReg&#34; and model_name != &#34;LinRegNet&#34;:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;LinRegNet&#34;))
        model_name = &#34;LinRegNet&#34;

    if data_type == &#34;LogReg&#34; and model_name != &#34;LogRegNet&#34;:
        warning_msg = &#34;WARNING: Dataset {} and model {} are not compatible. Setting model to {}.&#34;
        print(warning_msg.format(data_type, model_name, &#34;LogRegNet&#34;))
        model_name = &#34;LogRegNet&#34;

    return data_type, model_name, optim_name</code></pre>
</details>
</dd>
<dt id="src.utils.multivariate_normal"><code class="name flex">
<span>def <span class="ident">multivariate_normal</span></span>(<span>mean, cov, size=None, check_valid='warn', tol=1e-08)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw random samples from a multivariate normal distribution.</p>
<p>The multivariate normal, multinormal or Gaussian distribution is a
generalization of the one-dimensional normal distribution to higher
dimensions.
Such a distribution is specified by its mean and
covariance matrix.
These parameters are analogous to the mean
(average or "center") and variance (standard deviation, or "width,"
squared) of the one-dimensional normal distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code><a title="src.utils.multivariate_normal" href="#src.utils.multivariate_normal">RandomState.multivariate_normal()</a></code> method of a <code>default_rng()</code>
instance instead; please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>1-D array_like,</code> of <code>length N</code></dt>
<dd>Mean of the N-dimensional distribution.</dd>
<dt><strong><code>cov</code></strong> :&ensp;<code>2-D array_like,</code> of <code>shape (N, N)</code></dt>
<dd>Covariance matrix of the distribution. It must be symmetric and
positive-semidefinite for proper sampling.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Given a shape of, for example, <code>(m,n,k)</code>, <code>m*n*k</code> samples are
generated, and packed in an <code>m</code>-by-<code>n</code>-by-<code>k</code> arrangement.
Because
each sample is <code>N</code>-dimensional, the output shape is <code>(m,n,k,N)</code>.
If no shape is specified, a single (<code>N</code>-D) sample is returned.</dd>
<dt><strong><code>check_valid</code></strong> :&ensp;<code>{ 'warn', 'raise', 'ignore' }</code>, optional</dt>
<dd>Behavior when the covariance matrix is not positive semidefinite.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Tolerance when checking the singular values in covariance matrix.
cov is cast to double before the check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>
<p>The drawn samples, of shape <em>size</em>, if that was provided.
If not,
the shape is <code>(N,)</code>.</p>
<p>In other words, each entry <code>out[i,j,...,:]</code> is an N-dimensional
value drawn from the distribution.</p>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>Generator.multivariate_normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The mean is a coordinate in N-dimensional space, which represents the
location where samples are most likely to be generated.
This is
analogous to the peak of the bell curve for the one-dimensional or
univariate normal distribution.</p>
<p>Covariance indicates the level to which two variables vary together.
From the multivariate normal distribution, we draw N-dimensional
samples, :math:<code>X = [x_1, x_2, ... x_N]</code>.
The covariance matrix
element :math:<code>C_{ij}</code> is the covariance of :math:<code>x_i</code> and :math:<code>x_j</code>.
The element :math:<code>C_{ii}</code> is the variance of :math:<code>x_i</code> (i.e. its
"spread").</p>
<p>Instead of specifying the full covariance matrix, popular
approximations include:</p>
<ul>
<li>Spherical covariance (<code>cov</code> is a multiple of the identity matrix)</li>
<li>Diagonal covariance (<code>cov</code> has non-negative elements, and only on
the diagonal)</li>
</ul>
<p>This geometrical property can be seen in two dimensions by plotting
generated data-points:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mean = [0, 0]
&gt;&gt;&gt; cov = [[1, 0], [0, 100]]  # diagonal covariance
</code></pre>
<p>Diagonal covariance means that points are oriented along x or y-axis:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; x, y = np.random.multivariate_normal(mean, cov, 5000).T
&gt;&gt;&gt; plt.plot(x, y, 'x')
&gt;&gt;&gt; plt.axis('equal')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Note that the covariance matrix must be positive semidefinite (a.k.a.
nonnegative-definite). Otherwise, the behavior of this method is
undefined and backwards compatibility is not guaranteed.</p>
<h2 id="references">References</h2>
<p>.. [1] Papoulis, A., "Probability, Random Variables, and Stochastic
Processes," 3rd ed., New York: McGraw-Hill, 1991.
.. [2] Duda, R. O., Hart, P. E., and Stork, D. G., "Pattern
Classification," 2nd ed., New York: Wiley, 2001.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; mean = (1, 2)
&gt;&gt;&gt; cov = [[1, 0], [0, 1]]
&gt;&gt;&gt; x = np.random.multivariate_normal(mean, cov, (3, 3))
&gt;&gt;&gt; x.shape
(3, 3, 2)
</code></pre>
<p>The following is probably true, given that 0.6 is roughly twice the
standard deviation:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; list((x[0,0,:] - mean) &lt; 0.6)
[True, True] # random
</code></pre></div>
</dd>
<dt id="src.utils.normal"><code class="name flex">
<span>def <span class="ident">normal</span></span>(<span>loc=0.0, scale=1.0, size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw random samples from a normal (Gaussian) distribution.</p>
<p>The probability density function of the normal distribution, first
derived by De Moivre and 200 years later by both Gauss and Laplace
independently [2]_, is often called the bell curve because of
its characteristic shape (see the example below).</p>
<p>The normal distributions occurs often in nature.
For example, it
describes the commonly occurring distribution of samples influenced
by a large number of tiny, random disturbances, each with its own
unique distribution [2]_.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code><a title="src.utils.normal" href="#src.utils.normal">RandomState.normal()</a></code> method of a <code>default_rng()</code>
instance instead; please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loc</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Mean ("centre") of the distribution.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code> or <code>array_like</code> of <code>floats</code></dt>
<dd>Standard deviation (spread or "width") of the distribution. Must be
non-negative.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>ints</code>, optional</dt>
<dd>Output shape.
If the given shape is, e.g., <code>(m, n, k)</code>, then
<code>m * n * k</code> samples are drawn.
If size is <code>None</code> (default),
a single value is returned if <code>loc</code> and <code>scale</code> are both scalars.
Otherwise, <code>np.broadcast(loc, scale).size</code> samples are drawn.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code> or <code>scalar</code></dt>
<dd>Drawn samples from the parameterized normal distribution.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>scipy.stats.norm</code></dt>
<dd>probability density function, distribution or cumulative density function, etc.</dd>
<dt><code>Generator.normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The probability density for the Gaussian distribution is</p>
<p><span><span class="MathJax_Preview"> e^{ - \frac{ (x - \mu)^2 } {2 \sigma^2} }, </span><script type="math/tex; mode=display"> e^{ - \frac{ (x - \mu)^2 } {2 \sigma^2} }, </script></span>
where :math:<code>\mu</code> is the mean and :math:<code>\sigma</code> the standard
deviation. The square of the standard deviation, :math:<code>\sigma^2</code>,
is called the variance.</p>
<p>The function has its peak at the mean, and its "spread" increases with
the standard deviation (the function reaches 0.607 times its maximum at
:math:<code>x + \sigma</code> and :math:<code>x - \sigma</code> [2]_).
This implies that
normal is more likely to return samples lying close to the mean, rather
than those far away.</p>
<h2 id="references">References</h2>
<p>.. [1] Wikipedia, "Normal distribution",
<a href="https://en.wikipedia.org/wiki/Normal_distribution">https://en.wikipedia.org/wiki/Normal_distribution</a>
.. [2] P. R. Peebles Jr., "Central Limit Theorem" in "Probability,
Random Variables and Random Signal Principles", 4th ed., 2001,
pp. 51, 51, 125.</p>
<h2 id="examples">Examples</h2>
<p>Draw samples from the distribution:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; mu, sigma = 0, 0.1 # mean and standard deviation
&gt;&gt;&gt; s = np.random.normal(mu, sigma, 1000)
</code></pre>
<p>Verify the mean and the variance:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; abs(mu - np.mean(s))
0.0  # may vary
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; abs(sigma - np.std(s, ddof=1))
0.1  # may vary
</code></pre>
<p>Display the histogram of the samples, along with
the probability density function:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; count, bins, ignored = plt.hist(s, 30, density=True)
&gt;&gt;&gt; plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
...          linewidth=2, color='r')
&gt;&gt;&gt; plt.show()
</code></pre>
<p>Two-by-four array of samples from N(3, 6.25):</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.normal(3, 2.5, size=(2, 4))
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random
       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random
</code></pre></div>
</dd>
<dt id="src.utils.randn"><code class="name flex">
<span>def <span class="ident">randn</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>randn(d0, d1, &hellip;, dn)</p>
<p>Return a sample (or samples) from the "standard normal" distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a convenience function for users porting code from Matlab,
and wraps <code>standard_normal</code>. That function takes a
tuple to specify the size of the output, which is consistent with
other NumPy functions like <code>numpy.zeros</code> and <code>numpy.ones</code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>standard_normal</code> method of a <code>default_rng()</code>
instance instead; please see the :ref:<code>random-quick-start</code>.</p>
</div>
<p>If positive int_like arguments are provided, <code><a title="src.utils.randn" href="#src.utils.randn">RandomState.randn()</a></code> generates an array
of shape <code>(d0, d1, &hellip;, dn)</code>, filled
with random floats sampled from a univariate "normal" (Gaussian)
distribution of mean 0 and variance 1. A single float randomly sampled
from the distribution is returned if no argument is provided.</p>
<h2 id="parameters">Parameters</h2>
<p>d0, d1, &hellip;, dn : int, optional
The dimensions of the returned array, must be non-negative.
If no argument is given a single Python float is returned.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Z</code></strong> :&ensp;<code>ndarray</code> or <code>float</code></dt>
<dd>A <code>(d0, d1, &hellip;, dn)</code>-shaped array of floating-point samples from
the standard normal distribution, or a single such float if
no parameters were supplied.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>standard_normal</code></dt>
<dd>Similar, but takes a tuple as its argument.</dd>
<dt><code><a title="src.utils.normal" href="#src.utils.normal">RandomState.normal()</a></code></dt>
<dd>Also accepts mu and sigma arguments.</dd>
<dt><code>Generator.standard_normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For random samples from :math:<code>N(\mu, \sigma^2)</code>, use:</p>
<p><code>sigma * np.random.randn(...) + mu</code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randn()
2.1923875335537315  # random
</code></pre>
<p>Two-by-four array of samples from N(3, 6.25):</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random
       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random
</code></pre></div>
</dd>
<dt id="src.utils.restricted_float"><code class="name flex">
<span>def <span class="ident">restricted_float</span></span>(<span>x: str) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>An easy war to handle range for floats.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>float</code></dt>
<dd>Argument value.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>float</code></dt>
<dd>Terminal value if <strong>x</strong> is between zero and one.</dd>
</dl>
<h2 id="raises">Raises</h2>
<p><code>argparse.ArgumentTypeError</code>
If <strong>x</strong> cannot be converted to a float or is not in specified range.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restricted_float(x: str) -&gt; float:
    &#34;&#34;&#34;
    An easy war to handle range for floats.

    Parameters
    ----------
    x : float
        Argument value.

    Returns
    -------
    y : float
        Terminal value if **x** is between zero and one.

    Raises
    ------
    `argparse.ArgumentTypeError`
        If **x** cannot be converted to a float or is not in specified range.
    &#34;&#34;&#34;
    try:
        y = float(x)
    except ValueError:
        raise argparse.ArgumentTypeError(&#34;{} not a floating-point literal.&#34;.format(x))

    if y &lt; 0. or y &gt; 1.:
        raise argparse.ArgumentTypeError(&#34;{} not in range [0.0, 1.0].&#34;.format(y))

    return y</code></pre>
</details>
</dd>
<dt id="src.utils.set_seed"><code class="name flex">
<span>def <span class="ident">set_seed</span></span>(<span>seed)</span>
</code></dt>
<dd>
<div class="desc"><p>Fix seed for current run.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Global seeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_seed(seed):
    &#34;&#34;&#34;
    Fix seed for current run.

    Parameters
    ----------
    seed : int
        Global seeed.
    &#34;&#34;&#34;
    np.random.seed(seed)
    torch.manual_seed(seed)</code></pre>
</details>
</dd>
<dt id="src.utils.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>t: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Sigmoid function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>t</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Inputs.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sig_t</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Sigmoid of inputs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoid(t: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Sigmoid function.

    Parameters
    ----------
    t : np.ndarray
        Inputs.

    Returns
    -------
    sig_t : np.ndarray
        Sigmoid of inputs.
    &#34;&#34;&#34;
    sig_t = np.zeros(t.shape)

    # Separate where t is nonnegative
    pos_indices = t &gt; 0

    sig_t[pos_indices] = 1/(1 + np.exp(-t[pos_indices]))
    exp_t = np.exp(t[~pos_indices])
    sig_t[~pos_indices] = exp_t/(1 + exp_t)

    return sig_t</code></pre>
</details>
</dd>
<dt id="src.utils.simu_linreg"><code class="name flex">
<span>def <span class="ident">simu_linreg</span></span>(<span>n_features: int = 20, n_samples: int = 10000, corr: float = 0.5, std: float = 0.5) ‑> Tuple[Tuple[numpy.ndarray, ...], ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Simulation of a linear regression model with Gaussian features and a Toeplitz covariance, with
Gaussian noise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features to simulate.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>Number of samples to simulate.</dd>
<dt><strong><code>corr</code></strong> :&ensp;<code>float</code>, default=<code>0.5</code></dt>
<dd>Correlation of the features.</dd>
<dt><strong><code>std</code></strong> :&ensp;<code>float</code>, default=<code>0.5</code></dt>
<dd>Standard deviation of the noise.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>train_set</code></strong> :&ensp;<code>Tuple[np.ndarray]</code></dt>
<dd>Training data.</dd>
<dt><strong><code>test_set</code></strong> :&ensp;<code>Tuple[np.ndarray]</code></dt>
<dd>Test data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simu_linreg(n_features: int = 20, n_samples: int = 10000, corr: float = 0.5,
                std: float = 0.5) -&gt; Tuple[Tuple[np.ndarray, ...], ...]:
    &#34;&#34;&#34;
    Simulation of a linear regression model with Gaussian features and a Toeplitz covariance, with
    Gaussian noise.

    Parameters
    ----------
    n_features : int
        Number of features to simulate.

    n_samples : int, default=1000
        Number of samples to simulate.

    corr : float, default=0.5
        Correlation of the features.

    std : float, default=0.5
        Standard deviation of the noise.

    Returns
    -------
    train_set : Tuple[np.ndarray]
        Training data.

    test_set : Tuple[np.ndarray]
        Test data.
    &#34;&#34;&#34;
    # Weight creation
    w0 = normal(loc=2, scale=1, size=n_features)

    # Construction of a covariance matrix
    cov = toeplitz(corr ** np.arange(0, n_features))

    # Simulation of features
    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)

    # Simulation of the labels
    y = X.dot(w0) + std * randn(n_samples)

    # Create training and test sets
    train_size = int(0.8*n_samples)

    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    train_set = (X_train, y_train)
    test_set = (X_test, y_test)

    return train_set, test_set</code></pre>
</details>
</dd>
<dt id="src.utils.simu_logreg"><code class="name flex">
<span>def <span class="ident">simu_logreg</span></span>(<span>n_features: int = 20, n_samples: int = 10000, corr: float = 0.5) ‑> Tuple[Tuple[numpy.ndarray, ...], ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Simulation of a logistic regression model with Gaussian features and a Toeplitz covariance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features to simulate.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>Number of samples to simulate.</dd>
<dt><strong><code>corr</code></strong> :&ensp;<code>float</code>, default=<code>0.5</code></dt>
<dd>Correlation of the features.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>train_set</code></strong> :&ensp;<code>Tuple[np.ndarray]</code></dt>
<dd>Training data.</dd>
<dt><strong><code>test_set</code></strong> :&ensp;<code>Tuple[np.ndarray]</code></dt>
<dd>Test data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simu_logreg(n_features: int = 20, n_samples: int = 10000,
                corr: float = 0.5) -&gt; Tuple[Tuple[np.ndarray, ...], ...]:
    &#34;&#34;&#34;
    Simulation of a logistic regression model with Gaussian features and a Toeplitz covariance.

    Parameters
    ----------
    n_features : int
        Number of features to simulate.

    n_samples : int, default=1000
        Number of samples to simulate.

    corr : float, default=0.5
        Correlation of the features.

    Returns
    -------
    train_set : Tuple[np.ndarray]
        Training data.

    test_set : Tuple[np.ndarray]
        Test data.
    &#34;&#34;&#34;
    # Weight creation
    w0 = normal(loc=2, scale=1, size=n_features)

    # Construction of a covariance matrix
    cov = toeplitz(corr ** np.arange(0, n_features))

    # Simulation of features
    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)

    # Simulation of the labels
    p = sigmoid(X.dot(w0))
    y = np.random.binomial(1, p, size=n_samples)

    # Create training and test sets
    train_size = int(0.8*n_samples)

    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    train_set = (X_train, y_train)
    test_set = (X_test, y_test)

    return train_set, test_set</code></pre>
</details>
</dd>
<dt id="src.utils.str2bool"><code class="name flex">
<span>def <span class="ident">str2bool</span></span>(<span>v: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>An easy way to handle boolean options.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>v</code></strong> :&ensp;<code>str</code></dt>
<dd>Argument value.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str2bool(v) : bool</code></dt>
<dd>Corresponding boolean value, if it exists.</dd>
</dl>
<h2 id="raises">Raises</h2>
<p><code>argparse.ArgumentTypeError</code>
If the entry cannot be converted to a boolean.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def str2bool(v: str) -&gt; bool:
    &#34;&#34;&#34;
    An easy way to handle boolean options.

    Parameters
    ----------
    v : str
        Argument value.

    Returns
    -------
    str2bool(v) : bool
        Corresponding boolean value, if it exists.

    Raises
    ------
    `argparse.ArgumentTypeError`
        If the entry cannot be converted to a boolean.
    &#34;&#34;&#34;
    if isinstance(v, bool):
        return v
    if v.lower() in (&#34;yes&#34;, &#34;true&#34;, &#34;t&#34;, &#34;y&#34;, &#34;1&#34;):
        return True
    if v.lower() in (&#34;no&#34;, &#34;false&#34;, &#34;f&#34;, &#34;n&#34;, &#34;0&#34;):
        return False
    raise argparse.ArgumentTypeError(&#34;Boolean value expected.&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.utils.LinRegDataset"><code class="flex name class">
<span>class <span class="ident">LinRegDataset</span></span>
<span>(</span><span>data: Tuple[numpy.ndarray, ...])</span>
</code></dt>
<dd>
<div class="desc"><p>Dataset of generated linear regression data.</p>
<p>Initialize a dataset of linear regression data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Tuple[np.ndarray, &hellip;]</code></dt>
<dd>Tuple containing the samples and corresponding labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinRegDataset(Dataset):
    &#34;&#34;&#34;Dataset of generated linear regression data.&#34;&#34;&#34;

    def __init__(self, data: Tuple[np.ndarray, ...]):
        &#34;&#34;&#34;
        Initialize a dataset of linear regression data.

        Parameters
        ----------
        data : Tuple[np.ndarray, ...]
            Tuple containing the samples and corresponding labels.
        &#34;&#34;&#34;
        X, y = data
        self.X = torch.from_numpy(X.astype(np.float32))
        self.y = torch.from_numpy(y.astype(np.float32))

        self.transform = None

    def __len__(self) -&gt; int:
        &#34;&#34;&#34;
        Return the length of the dataset.

        Returns
        -------
        length : int
            Length of the dataset.
        &#34;&#34;&#34;
        length = len(self.y)

        return length

    def __getitem__(self, index) -&gt; Tuple[torch.Tensor, ...]:
        &#34;&#34;&#34;
        Return data point(s) at position **idx**.

        Parameters
        ----------
        index : Union[int, List[int]]
            Index or list of indices of selected data points.
        &#34;&#34;&#34;
        samples = self.X[index]
        labels = self.y[index].flatten()

        if self.transform:
            samples = self.transform(samples)
            labels = self.transform(labels)

        return samples, labels</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.utils.LinRegDataset.functions"><code class="name">var <span class="ident">functions</span> : Dict[str, Callable]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.utils.LogRegDataset"><code class="flex name class">
<span>class <span class="ident">LogRegDataset</span></span>
<span>(</span><span>data: Tuple[numpy.ndarray, ...])</span>
</code></dt>
<dd>
<div class="desc"><p>Dataset of generated logistic regression data.</p>
<p>Initialize a dataset of logistic regression data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Tuple[np.ndarray, &hellip;]</code></dt>
<dd>Tuple containing the samples and corresponding labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogRegDataset(Dataset):
    &#34;&#34;&#34;Dataset of generated logistic regression data.&#34;&#34;&#34;

    def __init__(self, data: Tuple[np.ndarray, ...]):
        &#34;&#34;&#34;
        Initialize a dataset of logistic regression data.

        Parameters
        ----------
        data : Tuple[np.ndarray, ...]
            Tuple containing the samples and corresponding labels.
        &#34;&#34;&#34;
        X, y = data
        self.X = torch.from_numpy(X.astype(np.float32))
        self.y = torch.from_numpy(y)

        self.transform = None

    def __len__(self) -&gt; int:
        &#34;&#34;&#34;
        Return the length of the dataset.

        Returns
        -------
        length : int
            Length of the dataset.
        &#34;&#34;&#34;
        length = len(self.y)

        return length

    def __getitem__(self, index) -&gt; Tuple[torch.Tensor, ...]:
        &#34;&#34;&#34;
        Return data point(s) at position **idx**.

        Parameters
        ----------
        index : Union[int, List[int]]
            Index or list of indices of selected data points.
        &#34;&#34;&#34;
        samples = self.X[index]
        labels = self.y[index].flatten()

        if self.transform:
            samples = self.transform(samples)
            labels = self.transform(labels)

        return samples, labels</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.utils.LogRegDataset.functions"><code class="name">var <span class="ident">functions</span> : Dict[str, Callable]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.utils.Partition"><code class="flex name class">
<span>class <span class="ident">Partition</span></span>
<span>(</span><span>data: torch.utils.data.dataset.Dataset, indices: List[int])</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper around a dataset and indices for a partition.</p>
<p>Initialize a partition of a dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>torch.utils.data.Dataset</code></dt>
<dd>Complete dataset.</dd>
<dt><strong><code>indices</code></strong> :&ensp;<code>List[int]</code></dt>
<dd>List of indices of the dataset that can be used in this partition.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Partition(Dataset):
    &#34;&#34;&#34;Wrapper around a dataset and indices for a partition.&#34;&#34;&#34;

    def __init__(self, data: Dataset, indices: List[int]):
        &#34;&#34;&#34;
        Initialize a partition of a dataset.

        Parameters
        ----------
        data : `torch.utils.data.Dataset`
            Complete dataset.

        indices : List[int]
            List of indices of the dataset that can be used in this partition.
        &#34;&#34;&#34;
        self.data = data
        self.indices = indices

    def __len__(self) -&gt; int:
        &#34;&#34;&#34;
        Return the length of the partition.

        Returns
        -------
        length : int
            Length of the partition.
        &#34;&#34;&#34;
        length = len(self.indices)
        return length

    def __getitem__(self, index) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Return data point(s) at position **index**.

        Parameters
        ----------
        index : Union[int, List[int]]
            Index or list of indices of selected data points.
        &#34;&#34;&#34;
        data_idx = self.indices[index]
        return self.data[data_idx]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.utils.Partition.functions"><code class="name">var <span class="ident">functions</span> : Dict[str, Callable]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.utils.build_train_test_set" href="#src.utils.build_train_test_set">build_train_test_set</a></code></li>
<li><code><a title="src.utils.check_params" href="#src.utils.check_params">check_params</a></code></li>
<li><code><a title="src.utils.multivariate_normal" href="#src.utils.multivariate_normal">multivariate_normal</a></code></li>
<li><code><a title="src.utils.normal" href="#src.utils.normal">normal</a></code></li>
<li><code><a title="src.utils.randn" href="#src.utils.randn">randn</a></code></li>
<li><code><a title="src.utils.restricted_float" href="#src.utils.restricted_float">restricted_float</a></code></li>
<li><code><a title="src.utils.set_seed" href="#src.utils.set_seed">set_seed</a></code></li>
<li><code><a title="src.utils.sigmoid" href="#src.utils.sigmoid">sigmoid</a></code></li>
<li><code><a title="src.utils.simu_linreg" href="#src.utils.simu_linreg">simu_linreg</a></code></li>
<li><code><a title="src.utils.simu_logreg" href="#src.utils.simu_logreg">simu_logreg</a></code></li>
<li><code><a title="src.utils.str2bool" href="#src.utils.str2bool">str2bool</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.utils.LinRegDataset" href="#src.utils.LinRegDataset">LinRegDataset</a></code></h4>
<ul class="">
<li><code><a title="src.utils.LinRegDataset.functions" href="#src.utils.LinRegDataset.functions">functions</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.utils.LogRegDataset" href="#src.utils.LogRegDataset">LogRegDataset</a></code></h4>
<ul class="">
<li><code><a title="src.utils.LogRegDataset.functions" href="#src.utils.LogRegDataset.functions">functions</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.utils.Partition" href="#src.utils.Partition">Partition</a></code></h4>
<ul class="">
<li><code><a title="src.utils.Partition.functions" href="#src.utils.Partition.functions">functions</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>