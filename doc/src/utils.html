<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.utils API documentation</title>
<meta name="description" content="Gather utilitary functions for randomness control and parameters checking." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.utils</code></h1>
</header>
<section id="section-intro">
<p>Gather utilitary functions for randomness control and parameters checking.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Gather utilitary functions for randomness control and parameters checking.&#34;&#34;&#34;


from typing import List, Tuple

import argparse

import random
import numpy as np
from numpy import ndarray

import torch
from torch.nn import Module
from torch.nn import CrossEntropyLoss, MSELoss
from torch.optim import Optimizer


from datasets import PartitionGenerator
from nn import LinRegNet, LogRegNet, MNISTNet, TorchNet, ResNet18, ResNet50
from optim import DistSGD, Signum


def set_seed(seed: int):
    &#34;&#34;&#34;Fix seed for current run.

    Parameters
    ----------
    seed : int
        Global seeed.
    &#34;&#34;&#34;
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)


def str2bool(v: str) -&gt; bool:
    &#34;&#34;&#34;An easy way to handle boolean options.

    Parameters
    ----------
    v : str
        Argument value.

    Returns
    -------
    str2bool(v) : bool
        Corresponding boolean value, if it exists.

    Raises
    ------
    argparse.ArgumentTypeError
        If the entry cannot be converted to a boolean.
    &#34;&#34;&#34;
    if isinstance(v, bool):
        return v
    if v.lower() in (&#34;yes&#34;, &#34;true&#34;, &#34;t&#34;, &#34;y&#34;, &#34;1&#34;):
        return True
    if v.lower() in (&#34;no&#34;, &#34;false&#34;, &#34;f&#34;, &#34;n&#34;, &#34;0&#34;):
        return False
    raise argparse.ArgumentTypeError(&#34;Boolean value expected.&#34;)


def check_nn(data_type: str, model_name: str, optim_name: str) -&gt; Tuple[str, ...]:
    &#34;&#34;&#34;Check whether chosen parameters for the nn are compatible.

    Parameters
    ----------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    Returns
    -------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    task : {&#34;classification&#34;, &#34;regression&#34;}
        If &#34;classification&#34; task, will compute the accuracy, otherwise only the loss.

    Raises
    ------
    ValueError
        If some of the parameters are not supported.
    &#34;&#34;&#34;
    # Unknown parameters
    available_nn = {&#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;, &#34;linregnet&#34;, &#34;logregnet&#34;}
    if model_name not in available_nn:
        error_msg = f&#34;Neural network {model_name} checking error.&#34; 
        error_msg += f&#34; Please choose one from {available_nn}.&#34;
        raise ValueError(error_msg)

    available_data = {&#34;mnist&#34;, &#34;imagenet&#34;, &#34;linreg&#34;, &#34;logreg&#34;}
    if data_type not in available_data:
        error_msg = f&#34;Dataset {data_type} checking error. Please choose one from {available_data}.&#34;
        raise ValueError(error_msg)

    availabe_optim = {&#34;distsgd&#34;, &#34;signum&#34;, &#34;signsgd&#34;}
    if optim_name not in availabe_optim:
        error_msg = f&#34;Optimizer {optim_name} is not supported.&#34;
        error_msg += f&#34; Please shoose one from {availabe_optim}.&#34;
        raise ValueError(error_msg)

    # Incompatible parameters
    if data_type == &#34;mnist&#34; and model_name not in {&#34;torchnet&#34;, &#34;mnistnet&#34;}:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to torchnet.&#34;
        print(warning_msg)
        model_name = &#34;torchnet&#34;

    if data_type == &#34;imagenet&#34; and model_name not in {&#34;resnet18&#34;, &#34;resnet50&#34;}:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to resnet18.&#34;
        print(warning_msg)
        model_name = &#34;resnet18&#34;

    if data_type == &#34;linreg&#34; and model_name != &#34;linregnet&#34;:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to linregnet.&#34;
        print(warning_msg)
        model_name = &#34;linregnet&#34;

    if data_type == &#34;logreg&#34; and model_name != &#34;logregnet&#34;:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to logregnet.&#34;
        print(warning_msg)
        model_name = &#34;logregnet&#34;

    # Task
    if data_type == &#34;linreg&#34;:
        task = &#34;regression&#34;
    else:
        task = &#34;classification&#34;

    return data_type, model_name, optim_name, task


def check_proc(world_size: int, blind_size: int, byz_size: int) -&gt; Tuple[int, ...]:
    &#34;&#34;&#34;Check whether chosen parameters for the processes are compatible.

    Parameters
    ----------
    world_size : int
        Total amount of processes.

    blind_size : int
       Number of blind adversaries that invert their gradients signs.

    byz_size : int
        Number of Byzantine adversaries.

    Returns
    -------
    world_size : int
        Total amount of processes.

    blind_size : int
       Number of blind adversaries that invert their gradients signs.

    byz_size : int
        Number of Byzantine adversaries.
    &#34;&#34;&#34;
    # Values range
    if world_size &lt;= 0:
        warning_msg = f&#34;WARNING: Number of processors was set to {world_size}. Changed it to 1.&#34;
        print(warning_msg)
        world_size = 1

    if blind_size &gt; world_size:
        warning_msg = &#34;WARNING: Number of blind adversaries greater than world size&#34;
        warning_msg += f&#34; ({blind_size} &gt; {world_size}). Changed it to {world_size}.&#34;
        print(warning_msg)
        blind_size = world_size

    left_size = world_size - blind_size
    if byz_size &gt; left_size:
        warning_msg = &#34;WARNING: Number of Byzantine adversaries greater than left ones&#34;
        warning_msg += f&#34; ({byz_size} &gt; {left_size}). Changed it to {left_size}.&#34;
        print(warning_msg)
        byz_size = left_size

    return world_size, blind_size, byz_size


def check_plot(metrics: List[str], task: str) -&gt; List[str]:
    &#34;&#34;&#34;Check that the metrics correspond to the task that was run.

    Parameters
    ----------
    metrics : list of str
        Names of the metrics to plot.

    task : {&#34;classification&#34;, &#34;regression&#34;}
        If &#34;classification&#34; task, will compute the accuracy, otherwise only the loss.

    Returns
    -------
    new_metrics : list of str
        Names of the metrics to plot.
    &#34;&#34;&#34;
    new_metrics = []

    for metric in metrics:

        if task == &#34;regression&#34; and metric == &#34;acc&#34;:
            print(&#34;WARNING: You wanted to plot accuracy for a regression task. Passed.&#34;)
        else:
            new_metrics.append(metric)

    return new_metrics


def init_model(model_name: str) -&gt; Module:
    &#34;&#34;&#34;Initialize a model based on its name.

    Parameters
    ----------
    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    Returns
    -------
    model : torch.nn.Module
        A neural network to be trained and evaluated on the datasets.
    &#34;&#34;&#34;
    model: Module

    if model_name == &#34;linregnet&#34;:
        model = LinRegNet()
    elif model_name == &#34;logregnet&#34;:
        model = LogRegNet()
    elif model_name == &#34;torchnet&#34;:
        model = TorchNet(10)
    elif model_name == &#34;mnistnet&#34;:
        model = MNISTNet(10)
    elif model_name == &#34;resnet18&#34;:
        model = ResNet18(10)
    elif model_name == &#34;resnet50&#34;:
        model = ResNet50(10)
    else:
        raise ValueError(f&#34;Unknown model {model_name}.&#34;)

    return model


def init_loss(data_type: str) -&gt; Module:
    &#34;&#34;&#34;Initialize a loss function based on the considered dataset.

    Parameters
    ----------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    Returns
    -------
    loss_fn : torch.nn.Module
        A loss function. Linear regression problem is associated to a mean squared error loss
        while the other datasets and models can be trained with the cross entropy loss.
    &#34;&#34;&#34;
    loss_fn : Module

    if data_type == &#34;linreg&#34;:
        loss_fn = MSELoss()
    else:
        loss_fn = CrossEntropyLoss()

    return loss_fn


def init_optim(optim_name: str, model: Module, blind_list: ndarray, byz_list: ndarray, 
               lr: float = 1e-3, momentum: float = 0.0, weight_decay: float = 0.0) -&gt; Optimizer:
    &#34;&#34;&#34;Initialize an optimizer based on its name.

    Parameters
    ----------
    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    blind_list : numpy.ndarray
        Ranks of blind adversaries inverting their gradients signs.

    byz_list : numpy.ndarray
        Ranks of Byzantine adversaries.

    lr : float, default=0.0001
        Learning rate for the optimizer.

    momentum : float, default=0.0
        Momentum for the optimizer.

    weight_decay : float, default=0.0
        Weight decay for the optimizer.

    Returns
    -------
    optimizer : torch.optim.Optimizer
        An optimizer to train the model with.
    &#34;&#34;&#34;
    optimizer: Optimizer

    if optim_name == &#34;distsgd&#34;:
        optimizer = DistSGD(model.parameters(), blind_list, byz_list, lr=lr, momentum=momentum)
    elif optim_name == &#34;signsgd&#34;:
        optimizer = Signum(model.parameters(), blind_list, byz_list, lr=lr, momentum=0.,
                           weight_decay=weight_decay)
    elif optim_name == &#34;signum&#34;:
        optimizer = Signum(model.parameters(), blind_list, byz_list, lr=lr, momentum=momentum,
                           weight_decay=weight_decay)
    else:
        raise ValueError(f&#34;Unknown optimizer {optim_name}.&#34;)

    return optimizer


def init_data(data_type: str) -&gt; PartitionGenerator:
    &#34;&#34;&#34;Initialize a dataset based on its name.

    Parameters
    ----------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    Returns
    -------
    data : `src.datasets.PartitionGenerator`
        An instance of `datasets.PartitionGenerator` to handle partitions for processes.
    &#34;&#34;&#34;
    data = PartitionGenerator(data_type, shuffle=True)

    return data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.utils.check_nn"><code class="name flex">
<span>def <span class="ident">check_nn</span></span>(<span>data_type: str, model_name: str, optim_name: str) ‑> Tuple[str, ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether chosen parameters for the nn are compatible.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"logreg", "linreg", "mnist", "imagenet"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"logregnet", "linregnet", "torchnet", "mnistnet", "resnet18", "resnet50"}</code></dt>
<dd>Name of the neural network to use for training.</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"distsgd", "signsgd", "signum"}</code></dt>
<dd>Name of the optimizer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"logreg", "linreg", "mnist", "imagenet"}</code></dt>
<dd>Name of the dataset.</dd>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"logregnet", "linregnet", "torchnet", "mnistnet", "resnet18", "resnet50"}</code></dt>
<dd>Name of the neural network to use for training.</dd>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"distsgd", "signsgd", "signum"}</code></dt>
<dd>Name of the optimizer.</dd>
<dt><strong><code>task</code></strong> :&ensp;<code>{"classification", "regression"}</code></dt>
<dd>If "classification" task, will compute the accuracy, otherwise only the loss.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If some of the parameters are not supported.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_nn(data_type: str, model_name: str, optim_name: str) -&gt; Tuple[str, ...]:
    &#34;&#34;&#34;Check whether chosen parameters for the nn are compatible.

    Parameters
    ----------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    Returns
    -------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    task : {&#34;classification&#34;, &#34;regression&#34;}
        If &#34;classification&#34; task, will compute the accuracy, otherwise only the loss.

    Raises
    ------
    ValueError
        If some of the parameters are not supported.
    &#34;&#34;&#34;
    # Unknown parameters
    available_nn = {&#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;, &#34;linregnet&#34;, &#34;logregnet&#34;}
    if model_name not in available_nn:
        error_msg = f&#34;Neural network {model_name} checking error.&#34; 
        error_msg += f&#34; Please choose one from {available_nn}.&#34;
        raise ValueError(error_msg)

    available_data = {&#34;mnist&#34;, &#34;imagenet&#34;, &#34;linreg&#34;, &#34;logreg&#34;}
    if data_type not in available_data:
        error_msg = f&#34;Dataset {data_type} checking error. Please choose one from {available_data}.&#34;
        raise ValueError(error_msg)

    availabe_optim = {&#34;distsgd&#34;, &#34;signum&#34;, &#34;signsgd&#34;}
    if optim_name not in availabe_optim:
        error_msg = f&#34;Optimizer {optim_name} is not supported.&#34;
        error_msg += f&#34; Please shoose one from {availabe_optim}.&#34;
        raise ValueError(error_msg)

    # Incompatible parameters
    if data_type == &#34;mnist&#34; and model_name not in {&#34;torchnet&#34;, &#34;mnistnet&#34;}:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to torchnet.&#34;
        print(warning_msg)
        model_name = &#34;torchnet&#34;

    if data_type == &#34;imagenet&#34; and model_name not in {&#34;resnet18&#34;, &#34;resnet50&#34;}:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to resnet18.&#34;
        print(warning_msg)
        model_name = &#34;resnet18&#34;

    if data_type == &#34;linreg&#34; and model_name != &#34;linregnet&#34;:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to linregnet.&#34;
        print(warning_msg)
        model_name = &#34;linregnet&#34;

    if data_type == &#34;logreg&#34; and model_name != &#34;logregnet&#34;:
        warning_msg = f&#34;WARNING: Dataset {data_type} and model {model_name} are not compatible.&#34;
        warning_msg += &#34; Setting model to logregnet.&#34;
        print(warning_msg)
        model_name = &#34;logregnet&#34;

    # Task
    if data_type == &#34;linreg&#34;:
        task = &#34;regression&#34;
    else:
        task = &#34;classification&#34;

    return data_type, model_name, optim_name, task</code></pre>
</details>
</dd>
<dt id="src.utils.check_plot"><code class="name flex">
<span>def <span class="ident">check_plot</span></span>(<span>metrics: List[str], task: str) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Check that the metrics correspond to the task that was run.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>metrics</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Names of the metrics to plot.</dd>
<dt><strong><code>task</code></strong> :&ensp;<code>{"classification", "regression"}</code></dt>
<dd>If "classification" task, will compute the accuracy, otherwise only the loss.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_metrics</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Names of the metrics to plot.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_plot(metrics: List[str], task: str) -&gt; List[str]:
    &#34;&#34;&#34;Check that the metrics correspond to the task that was run.

    Parameters
    ----------
    metrics : list of str
        Names of the metrics to plot.

    task : {&#34;classification&#34;, &#34;regression&#34;}
        If &#34;classification&#34; task, will compute the accuracy, otherwise only the loss.

    Returns
    -------
    new_metrics : list of str
        Names of the metrics to plot.
    &#34;&#34;&#34;
    new_metrics = []

    for metric in metrics:

        if task == &#34;regression&#34; and metric == &#34;acc&#34;:
            print(&#34;WARNING: You wanted to plot accuracy for a regression task. Passed.&#34;)
        else:
            new_metrics.append(metric)

    return new_metrics</code></pre>
</details>
</dd>
<dt id="src.utils.check_proc"><code class="name flex">
<span>def <span class="ident">check_proc</span></span>(<span>world_size: int, blind_size: int, byz_size: int) ‑> Tuple[int, ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether chosen parameters for the processes are compatible.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes.</dd>
<dt><strong><code>blind_size</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Number of blind adversaries that invert their gradients signs.</p>
<dl>
<dt><strong><code>byz_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of Byzantine adversaries.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>world_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Total amount of processes.</dd>
<dt><strong><code>blind_size</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Number of blind adversaries that invert their gradients signs.</p>
<dl>
<dt><strong><code>byz_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of Byzantine adversaries.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_proc(world_size: int, blind_size: int, byz_size: int) -&gt; Tuple[int, ...]:
    &#34;&#34;&#34;Check whether chosen parameters for the processes are compatible.

    Parameters
    ----------
    world_size : int
        Total amount of processes.

    blind_size : int
       Number of blind adversaries that invert their gradients signs.

    byz_size : int
        Number of Byzantine adversaries.

    Returns
    -------
    world_size : int
        Total amount of processes.

    blind_size : int
       Number of blind adversaries that invert their gradients signs.

    byz_size : int
        Number of Byzantine adversaries.
    &#34;&#34;&#34;
    # Values range
    if world_size &lt;= 0:
        warning_msg = f&#34;WARNING: Number of processors was set to {world_size}. Changed it to 1.&#34;
        print(warning_msg)
        world_size = 1

    if blind_size &gt; world_size:
        warning_msg = &#34;WARNING: Number of blind adversaries greater than world size&#34;
        warning_msg += f&#34; ({blind_size} &gt; {world_size}). Changed it to {world_size}.&#34;
        print(warning_msg)
        blind_size = world_size

    left_size = world_size - blind_size
    if byz_size &gt; left_size:
        warning_msg = &#34;WARNING: Number of Byzantine adversaries greater than left ones&#34;
        warning_msg += f&#34; ({byz_size} &gt; {left_size}). Changed it to {left_size}.&#34;
        print(warning_msg)
        byz_size = left_size

    return world_size, blind_size, byz_size</code></pre>
</details>
</dd>
<dt id="src.utils.init_data"><code class="name flex">
<span>def <span class="ident">init_data</span></span>(<span>data_type: str) ‑> datasets.partition_generator.PartitionGenerator</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize a dataset based on its name.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"logreg", "linreg", "mnist", "imagenet"}</code></dt>
<dd>Name of the dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code><a title="src.datasets.PartitionGenerator" href="datasets/index.html#src.datasets.PartitionGenerator">PartitionGenerator</a></code></dt>
<dd>An instance of <code>datasets.PartitionGenerator</code> to handle partitions for processes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_data(data_type: str) -&gt; PartitionGenerator:
    &#34;&#34;&#34;Initialize a dataset based on its name.

    Parameters
    ----------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    Returns
    -------
    data : `src.datasets.PartitionGenerator`
        An instance of `datasets.PartitionGenerator` to handle partitions for processes.
    &#34;&#34;&#34;
    data = PartitionGenerator(data_type, shuffle=True)

    return data</code></pre>
</details>
</dd>
<dt id="src.utils.init_loss"><code class="name flex">
<span>def <span class="ident">init_loss</span></span>(<span>data_type: str) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize a loss function based on the considered dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_type</code></strong> :&ensp;<code>{"logreg", "linreg", "mnist", "imagenet"}</code></dt>
<dd>Name of the dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loss_fn</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>A loss function. Linear regression problem is associated to a mean squared error loss
while the other datasets and models can be trained with the cross entropy loss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_loss(data_type: str) -&gt; Module:
    &#34;&#34;&#34;Initialize a loss function based on the considered dataset.

    Parameters
    ----------
    data_type : {&#34;logreg&#34;, &#34;linreg&#34;, &#34;mnist&#34;, &#34;imagenet&#34;}
        Name of the dataset.

    Returns
    -------
    loss_fn : torch.nn.Module
        A loss function. Linear regression problem is associated to a mean squared error loss
        while the other datasets and models can be trained with the cross entropy loss.
    &#34;&#34;&#34;
    loss_fn : Module

    if data_type == &#34;linreg&#34;:
        loss_fn = MSELoss()
    else:
        loss_fn = CrossEntropyLoss()

    return loss_fn</code></pre>
</details>
</dd>
<dt id="src.utils.init_model"><code class="name flex">
<span>def <span class="ident">init_model</span></span>(<span>model_name: str) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize a model based on its name.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>{"logregnet", "linregnet", "torchnet", "mnistnet", "resnet18", "resnet50"}</code></dt>
<dd>Name of the neural network to use for training.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>A neural network to be trained and evaluated on the datasets.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_model(model_name: str) -&gt; Module:
    &#34;&#34;&#34;Initialize a model based on its name.

    Parameters
    ----------
    model_name : {&#34;logregnet&#34;, &#34;linregnet&#34;, &#34;torchnet&#34;, &#34;mnistnet&#34;, &#34;resnet18&#34;, &#34;resnet50&#34;}
        Name of the neural network to use for training.

    Returns
    -------
    model : torch.nn.Module
        A neural network to be trained and evaluated on the datasets.
    &#34;&#34;&#34;
    model: Module

    if model_name == &#34;linregnet&#34;:
        model = LinRegNet()
    elif model_name == &#34;logregnet&#34;:
        model = LogRegNet()
    elif model_name == &#34;torchnet&#34;:
        model = TorchNet(10)
    elif model_name == &#34;mnistnet&#34;:
        model = MNISTNet(10)
    elif model_name == &#34;resnet18&#34;:
        model = ResNet18(10)
    elif model_name == &#34;resnet50&#34;:
        model = ResNet50(10)
    else:
        raise ValueError(f&#34;Unknown model {model_name}.&#34;)

    return model</code></pre>
</details>
</dd>
<dt id="src.utils.init_optim"><code class="name flex">
<span>def <span class="ident">init_optim</span></span>(<span>optim_name: str, model: torch.nn.modules.module.Module, blind_list: numpy.ndarray, byz_list: numpy.ndarray, lr: float = 0.001, momentum: float = 0.0, weight_decay: float = 0.0) ‑> torch.optim.optimizer.Optimizer</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize an optimizer based on its name.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>optim_name</code></strong> :&ensp;<code>{"distsgd", "signsgd", "signum"}</code></dt>
<dd>Name of the optimizer.</dd>
<dt><strong><code>blind_list</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Ranks of blind adversaries inverting their gradients signs.</dd>
<dt><strong><code>byz_list</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Ranks of Byzantine adversaries.</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code>, default=<code>0.0001</code></dt>
<dd>Learning rate for the optimizer.</dd>
<dt><strong><code>momentum</code></strong> :&ensp;<code>float</code>, default=<code>0.0</code></dt>
<dd>Momentum for the optimizer.</dd>
<dt><strong><code>weight_decay</code></strong> :&ensp;<code>float</code>, default=<code>0.0</code></dt>
<dd>Weight decay for the optimizer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>An optimizer to train the model with.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_optim(optim_name: str, model: Module, blind_list: ndarray, byz_list: ndarray, 
               lr: float = 1e-3, momentum: float = 0.0, weight_decay: float = 0.0) -&gt; Optimizer:
    &#34;&#34;&#34;Initialize an optimizer based on its name.

    Parameters
    ----------
    optim_name : {&#34;distsgd&#34;, &#34;signsgd&#34;, &#34;signum&#34;}
        Name of the optimizer.

    blind_list : numpy.ndarray
        Ranks of blind adversaries inverting their gradients signs.

    byz_list : numpy.ndarray
        Ranks of Byzantine adversaries.

    lr : float, default=0.0001
        Learning rate for the optimizer.

    momentum : float, default=0.0
        Momentum for the optimizer.

    weight_decay : float, default=0.0
        Weight decay for the optimizer.

    Returns
    -------
    optimizer : torch.optim.Optimizer
        An optimizer to train the model with.
    &#34;&#34;&#34;
    optimizer: Optimizer

    if optim_name == &#34;distsgd&#34;:
        optimizer = DistSGD(model.parameters(), blind_list, byz_list, lr=lr, momentum=momentum)
    elif optim_name == &#34;signsgd&#34;:
        optimizer = Signum(model.parameters(), blind_list, byz_list, lr=lr, momentum=0.,
                           weight_decay=weight_decay)
    elif optim_name == &#34;signum&#34;:
        optimizer = Signum(model.parameters(), blind_list, byz_list, lr=lr, momentum=momentum,
                           weight_decay=weight_decay)
    else:
        raise ValueError(f&#34;Unknown optimizer {optim_name}.&#34;)

    return optimizer</code></pre>
</details>
</dd>
<dt id="src.utils.set_seed"><code class="name flex">
<span>def <span class="ident">set_seed</span></span>(<span>seed: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Fix seed for current run.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Global seeed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_seed(seed: int):
    &#34;&#34;&#34;Fix seed for current run.

    Parameters
    ----------
    seed : int
        Global seeed.
    &#34;&#34;&#34;
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)</code></pre>
</details>
</dd>
<dt id="src.utils.str2bool"><code class="name flex">
<span>def <span class="ident">str2bool</span></span>(<span>v: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>An easy way to handle boolean options.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>v</code></strong> :&ensp;<code>str</code></dt>
<dd>Argument value.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str2bool(v) : bool</code></dt>
<dd>Corresponding boolean value, if it exists.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>argparse.ArgumentTypeError</code></dt>
<dd>If the entry cannot be converted to a boolean.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def str2bool(v: str) -&gt; bool:
    &#34;&#34;&#34;An easy way to handle boolean options.

    Parameters
    ----------
    v : str
        Argument value.

    Returns
    -------
    str2bool(v) : bool
        Corresponding boolean value, if it exists.

    Raises
    ------
    argparse.ArgumentTypeError
        If the entry cannot be converted to a boolean.
    &#34;&#34;&#34;
    if isinstance(v, bool):
        return v
    if v.lower() in (&#34;yes&#34;, &#34;true&#34;, &#34;t&#34;, &#34;y&#34;, &#34;1&#34;):
        return True
    if v.lower() in (&#34;no&#34;, &#34;false&#34;, &#34;f&#34;, &#34;n&#34;, &#34;0&#34;):
        return False
    raise argparse.ArgumentTypeError(&#34;Boolean value expected.&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="Directory index" href="index.html">
<img src="logo.png" alt="">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="src.utils.check_nn" href="#src.utils.check_nn">check_nn</a></code></li>
<li><code><a title="src.utils.check_plot" href="#src.utils.check_plot">check_plot</a></code></li>
<li><code><a title="src.utils.check_proc" href="#src.utils.check_proc">check_proc</a></code></li>
<li><code><a title="src.utils.init_data" href="#src.utils.init_data">init_data</a></code></li>
<li><code><a title="src.utils.init_loss" href="#src.utils.init_loss">init_loss</a></code></li>
<li><code><a title="src.utils.init_model" href="#src.utils.init_model">init_model</a></code></li>
<li><code><a title="src.utils.init_optim" href="#src.utils.init_optim">init_optim</a></code></li>
<li><code><a title="src.utils.set_seed" href="#src.utils.set_seed">set_seed</a></code></li>
<li><code><a title="src.utils.str2bool" href="#src.utils.str2bool">str2bool</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>